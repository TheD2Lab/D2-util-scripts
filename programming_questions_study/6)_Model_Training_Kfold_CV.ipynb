{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa3BV-E6X1a8"
      },
      "source": [
        "# 4. Answering RQ4\n",
        "\n",
        "**Performing k-fold cross validation on all models (Trained with DGMs + Digital Biomarkers vs DGMs alone**)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7atjUmY-XcsT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "import cv2\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Input, Concatenate, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9gRWRkkXmZ0"
      },
      "outputs": [],
      "source": [
        "CECS_698_PATH = '/content/drive/MyDrive/CECS 698 - Data Analysis/'\n",
        "\n",
        "PARTICIPANTS = [i for i in range(4, 27)]\n",
        "\n",
        "\n",
        "FPOG_SCANPATHS_PATH = os.path.join(CECS_698_PATH, 'FPOG Scanpaths')\n",
        "\n",
        "MERGED_DATA = os.path.join(CECS_698_PATH, 'Merged Data')\n",
        "\n",
        "GOOGLE_FORMS_SHEETS = os.path.join(CECS_698_PATH, 'Google Forms Sheets')\n",
        "\n",
        "GROUP = {\n",
        "    'E-H': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 24],\n",
        "    'H-E': [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26]\n",
        "}\n",
        "\n",
        "# Particpants ids to exclude on specific assessments\n",
        "EASY_ASSESSMENT_EXCLUDE = [11, 22]\n",
        "HARD_ASSESSMENT_EXCLUDE = [15, 21]\n",
        "\n",
        "# Size of image to store image files\n",
        "IMG_SIZE=(70, 70)\n",
        "\n",
        "N_SPLITS = 3\n",
        "DROP_COLUMNS = ['eda_scl_usiemens', 'pulse_rate_bpm', 'temperature_celsius']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMK9ckNI9QYC"
      },
      "source": [
        "# Gathering Data üìà\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toPEW2VMfxyg"
      },
      "source": [
        "## Getting Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWyapTpzXmpb"
      },
      "outputs": [],
      "source": [
        "df_scores = pd.read_csv(os.path.join(CECS_698_PATH, 'Participant Scores.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQ1bUuJRDOx"
      },
      "source": [
        "## Combining Questionnaire Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbSKYntQQnh2"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------- NASA TLX -----------------------------------------------\n",
        "df_NASA_TLX = pd.read_csv(os.path.join(GOOGLE_FORMS_SHEETS, 'Participants Sheet - NASA TLX.csv'))#.drop('Dry Run?', axis=1)\n",
        "\n",
        "df_NASA_TLX.columns = [\n",
        "    'Timestamp',\n",
        "    'Participant ID',\n",
        "    'Mental Demand', # 1\n",
        "    'Physical Demand', # 2\n",
        "    'Temporal Demand', # 3\n",
        "    'Performance', # 4\n",
        "    'Effort', # 5\n",
        "    'Frustration', # 6\n",
        "    'Dry Run', # 7\n",
        "]\n",
        "\n",
        "df_NASA_TLX = df_NASA_TLX[df_NASA_TLX['Dry Run'].isnull()] # Take out dry run participants\n",
        "df_NASA_TLX.drop(['Dry Run', 'Timestamp'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------- Pre-Study KSS -----------------------------------------------\n",
        "df_pre_study_KSS = pd.read_csv(os.path.join(GOOGLE_FORMS_SHEETS, 'Participants Sheet - Pre-Study KSS.csv'))\n",
        "df_pre_study_KSS = df_pre_study_KSS[df_pre_study_KSS['Dry Run?'].isnull()]\n",
        "df_pre_study_KSS.drop(['Dry Run?', 'Timestamp'], axis=1, inplace=True)\n",
        "df_pre_study_KSS.columns = [\n",
        "    'Participant ID',\n",
        "    'Hours Awake',\n",
        "    'Pre-Sleepiness Scale',\n",
        "]\n",
        "df_pre_study_KSS['Pre-Sleepiness Scale'] = df_pre_study_KSS['Pre-Sleepiness Scale'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "\n",
        "# ----------------------------------------------- Post-Study KSS -----------------------------------------------\n",
        "df_post_study_KSS = pd.read_csv(os.path.join(GOOGLE_FORMS_SHEETS, 'Participants Sheet - Post-Study KSS.csv'))\n",
        "df_post_study_KSS = df_post_study_KSS[df_post_study_KSS['Dry Run?'].isnull()]\n",
        "df_post_study_KSS.drop(['Dry Run?', 'Timestamp'], axis=1, inplace=True)\n",
        "df_post_study_KSS.columns = [\n",
        "    'Participant ID',\n",
        "    'Post-Sleepiness Scale',\n",
        "]\n",
        "df_post_study_KSS['Post-Sleepiness Scale'] = df_post_study_KSS['Post-Sleepiness Scale'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "df_all_questionnaires = df_NASA_TLX.merge(df_pre_study_KSS, on='Participant ID', how='inner').merge(df_post_study_KSS, on='Participant ID', how='inner')\n",
        "df_all_questionnaires = df_all_questionnaires.merge(df_scores, left_on='Participant ID', right_on='Participant ID ').drop(columns=['Participant ID '])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG45G-IhQ6qN",
        "outputId": "c5ad0e03-8cad-4561-ff07-01569386ca9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Participant ID', 'Mental Demand', 'Physical Demand', 'Temporal Demand',\n",
              "       'Performance', 'Effort', 'Frustration', 'Hours Awake',\n",
              "       'Pre-Sleepiness Scale', 'Post-Sleepiness Scale', 'Score', 'Assessment',\n",
              "       'Group', 'Elapsed Minutes', 'Successful/Unsuccessful',\n",
              "       'Python Experience', 'Year of Study'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_all_questionnaires.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYMGRtaP8XPr"
      },
      "source": [
        "## Get X, y data then `train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvhw7srtXsp_"
      },
      "outputs": [],
      "source": [
        "def read_y_labels(path):\n",
        "    y = []\n",
        "    for p in PARTICIPANTS:\n",
        "        for diff in ['easy', 'hard']:\n",
        "            if (diff == 'easy' and p in EASY_ASSESSMENT_EXCLUDE) or (diff == 'hard' and p in HARD_ASSESSMENT_EXCLUDE):\n",
        "                continue # Excluded assessments (i.e ones with data loss)\n",
        "\n",
        "            label = df_scores[(df_scores['Participant ID '] == p) & (df_scores['Assessment'] == diff)]['Successful/Unsuccessful'].values[0]\n",
        "            label = 1 if label == 'Successful' else 0\n",
        "            y.append(label)\n",
        "\n",
        "    y = np.array(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def read_X_timeseries(path, exclude_biomarkers=False):\n",
        "    X = []\n",
        "    for p in PARTICIPANTS:\n",
        "        for diff in ['easy', 'hard']:\n",
        "            if (diff == 'easy' and p in EASY_ASSESSMENT_EXCLUDE) or (diff == 'hard' and p in HARD_ASSESSMENT_EXCLUDE):\n",
        "                continue # Excluded assessments (i.e ones with data loss)\n",
        "\n",
        "            folder = f\"Participant {p}\"\n",
        "            file = f\"participant{p}_{diff}_assessment.csv\"\n",
        "\n",
        "            # Merged data path\n",
        "            df = pd.read_csv(os.path.join(path, folder, file))\n",
        "            df.set_index('timestamp_unix', inplace=True)\n",
        "\n",
        "            if exclude_biomarkers:\n",
        "                df.drop(columns=DROP_COLUMNS, inplace=True)\n",
        "\n",
        "            # Synthetic data path\n",
        "            X.append(df)\n",
        "\n",
        "    return X\n",
        "\n",
        "def read_X_categorical(path):\n",
        "    # Getting all dataframes and splitting them into 2 groups: Successful and Unsuccessful\n",
        "    X = pd.DataFrame()\n",
        "\n",
        "    features = ['Participant ID', 'Mental Demand', 'Physical Demand', 'Temporal Demand',\n",
        "       'Performance', 'Effort', 'Frustration', 'Hours Awake',\n",
        "       'Pre-Sleepiness Scale', 'Post-Sleepiness Scale', 'Assessment',\n",
        "       'Group', 'Elapsed Minutes',\n",
        "       'Python Experience', 'Year of Study']\n",
        "\n",
        "    for p in PARTICIPANTS:\n",
        "        for diff in ['easy', 'hard']:\n",
        "            if (diff == 'easy' and p in EASY_ASSESSMENT_EXCLUDE) or (diff == 'hard' and p in HARD_ASSESSMENT_EXCLUDE):\n",
        "                continue # Excluded assessments (i.e ones with data loss)\n",
        "\n",
        "            filter = (df_all_questionnaires['Participant ID'] == p) & (df_all_questionnaires['Assessment'] == diff)\n",
        "            df_participant = df_all_questionnaires[filter]\n",
        "\n",
        "            X = pd.concat([X, df_participant[features]], axis=0)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "# Helper function to read a single image\n",
        "def read_image(img_path, img_size):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, img_size) # Resizing all images to make them uniform\n",
        "    img = img.astype(\"float32\") / 255.0  # Normalize\n",
        "    return img\n",
        "\n",
        "def read_X_image(path, img_size=IMG_SIZE):\n",
        "    X = []\n",
        "\n",
        "    for p in PARTICIPANTS:\n",
        "        for diff in ['easy', 'hard']:\n",
        "            if (diff == 'easy' and p in EASY_ASSESSMENT_EXCLUDE) or (diff == 'hard' and p in HARD_ASSESSMENT_EXCLUDE):\n",
        "                continue # Excluded assessments (i.e ones with data loss)\n",
        "\n",
        "            folder = f\"Participant {p}\"\n",
        "            file = f\"FPOG {diff}.png\"\n",
        "\n",
        "            # Reading in each image\n",
        "            img_path = os.path.join(path, folder, file)\n",
        "            img = read_image(img_path, img_size)\n",
        "            X.append(img)\n",
        "\n",
        "    X = np.array(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfrL__nOonL"
      },
      "source": [
        "Train test splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO8pRN-xZwXO",
        "outputId": "a47a3e32-c299-4c74-ecf6-3c391a774779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_timeseries: 42, X_categorical: 42, X_image: 42, y_actual: 42\n"
          ]
        }
      ],
      "source": [
        "X_timeseries = read_X_timeseries(MERGED_DATA)\n",
        "X_categorical = read_X_categorical(MERGED_DATA)\n",
        "X_image = read_X_image(FPOG_SCANPATHS_PATH)\n",
        "y = read_y_labels(MERGED_DATA)\n",
        "\n",
        "print(f\"X_timeseries: {len(X_timeseries)}, X_categorical: {len(X_categorical)}, X_image: {len(X_image)}, y_actual: {len(y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_models = os.path.join(CECS_698_PATH, 'Saved Models 3')\n",
        "if not os.path.exists(saved_models):\n",
        "    os.makedirs(saved_models)\n",
        "    print(f\"Created {saved_models}\")\n",
        "else:\n",
        "    print(f\"{saved_models} already exists\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MCpd27hRTyt",
        "outputId": "5d9af7e1-f57d-4a01-faae-1a95c734003c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CECS 698 - Data Analysis/Saved Models 3 already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTPxS3k2R1bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7abnCKfXJeL"
      },
      "source": [
        "# Data Preprocessing üè≠\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6nu7NUS654L"
      },
      "source": [
        "## Function to Preprocess timeseries data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5KFhcZiyqWh"
      },
      "outputs": [],
      "source": [
        "def preprocess_timeseries(X_train, X_test):\n",
        "    X_train_preprocessed, X_test_preprocessed = [], []\n",
        "\n",
        "    # For each participant, fit MinMaxScaler on their own data.\n",
        "    for i, x in enumerate(X_train):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_values = scaler.fit_transform(x.values)\n",
        "        X_train_preprocessed.append(scaled_values)\n",
        "\n",
        "    for i, x in enumerate(X_test):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_values = scaler.fit_transform(x.values)\n",
        "        X_test_preprocessed.append(scaled_values)\n",
        "\n",
        "    # Pad sequences. To ensure consistency, we use the max length from the training set.\n",
        "    X_train_preprocessed = pad_sequences(X_train_preprocessed, dtype='float32', padding='post')\n",
        "    maxlen = X_train_preprocessed.shape[1]\n",
        "    X_test_preprocessed = pad_sequences(X_test_preprocessed, maxlen=maxlen, dtype='float32', padding='post') # Pad sequences to the max length (Expected: 248)\n",
        "\n",
        "    return X_train_preprocessed, X_test_preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svxNzBrA9rvz"
      },
      "source": [
        "## Function to Preprocess categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX_WGBfp3sCC"
      },
      "outputs": [],
      "source": [
        "def apply_ohe(X_train, X_test):\n",
        "    # One hot encoding (Year of Study and Assessment)\n",
        "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')  # `drop=first` to prevent multicollinearity\n",
        "    train_encoded = encoder.fit_transform(X_train[['Year of Study', 'Assessment']])\n",
        "    test_encoded = encoder.transform(X_test[['Year of Study', 'Assessment']])\n",
        "    train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(['Year of Study', 'Assessment']))\n",
        "    test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(['Year of Study', 'Assessment']))\n",
        "\n",
        "    # Resetting row indices\n",
        "    X_train.reset_index(drop=True, inplace=True)\n",
        "    X_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Drop original categorical columns and concatenate with encoded features\n",
        "    X_train = pd.concat([X_train.drop(columns=['Year of Study', 'Assessment']), train_encoded_df], axis=1)\n",
        "    X_test = pd.concat([X_test.drop(columns=['Year of Study', 'Assessment']), test_encoded_df], axis=1)\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def apply_scaling(X_train, X_test):\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_columns = [\n",
        "        'Mental Demand',\n",
        "        'Physical Demand',\n",
        "        'Temporal Demand',\n",
        "        'Performance',\n",
        "        'Effort',\n",
        "        'Frustration',\n",
        "        'Hours Awake',\n",
        "        'Elapsed Minutes',\n",
        "        'Pre-Sleepiness Scale',\n",
        "        'Post-Sleepiness Scale'\n",
        "    ]\n",
        "\n",
        "    X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
        "    X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def apply_ordinal(X_train, X_test):\n",
        "    experience_level = list(X_train['Python Experience'].unique())\n",
        "\n",
        "    encoder = OrdinalEncoder(categories=[experience_level])\n",
        "    X_train[['Python Experience']] = encoder.fit_transform(X_train[['Python Experience']])\n",
        "    X_test[['Python Experience']] = encoder.transform(X_test[['Python Experience']])\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def preprocess_categorical(X_train, X_test):\n",
        "    # We can change the `Year of Study` feature to divide between undergrad and graduate students strictly.\n",
        "    X_train['Year of Study'] = X_train['Year of Study'].apply(lambda x: 'Undergrad' if 'Undergrad' in x else 'Grad')\n",
        "    X_test['Year of Study'] = X_test['Year of Study'].apply(lambda x: 'Undergrad' if 'Undergrad' in x else 'Grad')\n",
        "\n",
        "    # One hot encoding\n",
        "    X_train, X_test = apply_ohe(X_train.copy(), X_test.copy())\n",
        "\n",
        "\n",
        "    # Dropping irrelevant columns from categorical data\n",
        "    X_train.drop(columns=['Group', 'Participant ID'], inplace=True)\n",
        "    X_train.reset_index(drop=True, inplace=True)\n",
        "    X_test.drop(columns=['Group', 'Participant ID'], inplace=True)\n",
        "    X_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # MinMaxScaling\n",
        "    X_train, X_test = apply_scaling(X_train.copy(), X_test.copy())\n",
        "\n",
        "    # Ordinal encoding\n",
        "    X_train, X_test = apply_ordinal(X_train.copy(), X_test.copy())\n",
        "\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-XN0e0nk6z8"
      },
      "source": [
        "## Function to Preprocess Image Data\n",
        "\n",
        "Rearranging the shape of train and test datasets to have one more dimension at the end representing color channels:\n",
        "\n",
        "(Number of Images x Height x Width x Color Channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjtTLHlM5f6H"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(X_train, X_test):\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "    X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zBzn1NmbKoS"
      },
      "source": [
        "# Model Training: Eye Tracking and Physiological Biomarkers ü´ÄüëÅÔ∏è\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6y6elNrClpA"
      },
      "source": [
        "## Single Modal: Timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJJTYsvT6kMV",
        "outputId": "2c7b7821-6bca-456e-b31e-510a4bbdd886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 14)\n",
            "(28, 14)\n",
            "(28, 14)\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    print(f\"{len(train_labels), len(test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBaEeNYi1vIS"
      },
      "outputs": [],
      "source": [
        "def build_model_single_modal():\n",
        "    num_features = X_timeseries[0].shape[1]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(None, num_features)))\n",
        "    # model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output for binary classification\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0oyaP0rqgrj",
        "outputId": "edd946b1-2cfe-4aef-96ac-b1b9e2f21bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.6941, Test Accuracy: 0.4286, Epochs: 11\n",
            "Fold 2 - Test Loss: 0.8691, Test Accuracy: 0.5714, Epochs: 17\n",
            "Fold 3 - Test Loss: 0.6932, Test Accuracy: 0.5000, Epochs: 12\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.5000, Loss: 0.7521\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'single_modal.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_single_modal()\n",
        "    history = model.fit(\n",
        "        train_time_series,\n",
        "        train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=(test_time_series, test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores = model.evaluate(test_time_series, test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSRzAuI1CMW8"
      },
      "source": [
        "## 2-Modal: Timeseries + Questionnaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sus51FQzCX6v"
      },
      "outputs": [],
      "source": [
        "def build_model_two_modal_ts_q():\n",
        "    # Time Series Input (LSTM)\n",
        "    num_timeseries_features = X_timeseries[0].shape[1]\n",
        "    time_input = Input(shape=(None, num_timeseries_features), name=\"Time_Series_Input\")\n",
        "    ts = LSTM(64, return_sequences=True)(time_input)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = LSTM(32, return_sequences=False)(ts)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = Dense(16, activation='relu')(ts)\n",
        "\n",
        "    # Questionnaire Input (Dense)\n",
        "    num_questionnaire_features = 13\n",
        "    questionnaire_input = Input(shape=(num_questionnaire_features,), name='Questionnaire_Input')\n",
        "    q = Dense(64, activation='relu')(questionnaire_input)\n",
        "    q = Dropout(0.2)(q)\n",
        "    q = Dense(32, activation='relu')(q)\n",
        "    q = Dropout(0.2)(q)\n",
        "    q = Dense(16, activation='relu')(q)\n",
        "\n",
        "    ## Merge all inputs\n",
        "    merged = Concatenate()([ts, q])\n",
        "    output = Dense(1, activation='sigmoid', name=\"Output\")(merged)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=[time_input, questionnaire_input], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu316JoEr1HL",
        "outputId": "0b8030ea-24d8-48fa-8658-304aeed83363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.6601, Test Accuracy: 0.5714, Epochs: 18\n",
            "Fold 2 - Test Loss: 0.6777, Test Accuracy: 0.7143, Epochs: 28\n",
            "Fold 3 - Test Loss: 0.6668, Test Accuracy: 0.7143, Epochs: 50\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6667, Loss: 0.6682\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'two_modal.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Categorical Data ----------------------------\n",
        "    train_categorical = X_categorical.iloc[train_index]\n",
        "    test_categorical = X_categorical.iloc[test_index]\n",
        "\n",
        "    train_categorical, test_categorical = preprocess_categorical(train_categorical.copy(), test_categorical.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_two_modal_ts_q()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_categorical], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_categorical], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_categorical], test_labels, verbose=0) # model.evaluate(test_time_padded, test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzxL_us0QZio"
      },
      "source": [
        "## 2-Modal: Timeseries + Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT8UJpB5QgGv"
      },
      "outputs": [],
      "source": [
        "def build_model_two_modal_ts_img():\n",
        "    # Time Series Input (LSTM)\n",
        "    num_timeseries_features = X_timeseries[0].shape[1]\n",
        "    time_input = Input(shape=(None, num_timeseries_features), name=\"Time_Series_Input\")\n",
        "    ts = LSTM(64, return_sequences=True)(time_input)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = LSTM(32, return_sequences=False)(ts)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = Dense(16, activation='relu')(ts)\n",
        "\n",
        "    # Image Input (CNN)\n",
        "    input_shape = (IMG_SIZE[0], IMG_SIZE[1], 1)\n",
        "    image_input = Input(shape=input_shape, name=\"Image_Input\")\n",
        "    img = Conv2D(64, (3, 3), activation='relu')(image_input)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Conv2D(32, (3, 3), activation='relu')(img)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Conv2D(16, (3, 3), activation='relu')(img)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Flatten()(img)\n",
        "    img = Dense(64, activation='relu')(img)\n",
        "\n",
        "    # Merge all inputs\n",
        "    merged = Concatenate()([ts, img])\n",
        "    output = Dense(1, activation='sigmoid', name=\"Output\")(merged)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=[time_input, image_input], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0_AyrwFPLFy",
        "outputId": "a19ea3f7-d91e-4512-e8d2-e9077bfe4c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.5595, Test Accuracy: 0.7857, Epochs: 17\n",
            "Fold 2 - Test Loss: 0.7086, Test Accuracy: 0.6429, Epochs: 11\n",
            "Fold 3 - Test Loss: 0.7167, Test Accuracy: 0.5714, Epochs: 11\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6667, Loss: 0.6616\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'two_modal2.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Image Data ----------------------------\n",
        "    train_image = []\n",
        "    for i in train_index:\n",
        "        train_image.append(X_image[i])\n",
        "\n",
        "    test_image = []\n",
        "    for i in test_index:\n",
        "        test_image.append(X_image[i])\n",
        "\n",
        "    train_image, test_image = preprocess_image(train_image.copy(), test_image.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_two_modal_ts_img()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_image], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_image], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_image], test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-evgKDwQCAC7"
      },
      "source": [
        "## 3-Modal: Timeseries + Questionnaires + Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0njykl9PdyCU"
      },
      "outputs": [],
      "source": [
        "def build_model_three_modal():\n",
        "    # Time Series Input (LSTM)\n",
        "    num_timeseries_features = X_timeseries[0].shape[1]\n",
        "    time_input = Input(shape=(None, num_timeseries_features), name=\"Time_Series_Input\")\n",
        "    ts = LSTM(64, return_sequences=True)(time_input)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = LSTM(32, return_sequences=False)(ts)\n",
        "    ts = Dropout(0.2)(ts)\n",
        "    ts = Dense(16, activation='relu')(ts)\n",
        "\n",
        "    ## Questionnaire Input (Dense)\n",
        "    num_questionnaire_features = 13\n",
        "    questionnaire_input = Input(shape=(num_questionnaire_features,), name='Questionnaire_Input')\n",
        "    q = Dense(64, activation='relu')(questionnaire_input)\n",
        "    q = Dropout(0.2)(q)\n",
        "    q = Dense(32, activation='relu')(q)\n",
        "    q = Dropout(0.2)(q)\n",
        "    q = Dense(16, activation='relu')(q)\n",
        "\n",
        "    # Image Input (CNN)\n",
        "    input_shape = (IMG_SIZE[0], IMG_SIZE[1], 1)\n",
        "    image_input = Input(shape=input_shape, name=\"Image_Input\")\n",
        "    img = Conv2D(64, (3, 3), activation='relu')(image_input)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Conv2D(32, (3, 3), activation='relu')(img)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Conv2D(16, (3, 3), activation='relu')(img)\n",
        "    img = MaxPooling2D((2, 2))(img)\n",
        "    img = Flatten()(img)\n",
        "    img = Dense(64, activation='relu')(img)\n",
        "\n",
        "    ## Merge all inputs\n",
        "    merged = Concatenate()([ts, q, img])\n",
        "    output = Dense(1, activation='sigmoid', name=\"Output\")(merged)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(inputs=[time_input, questionnaire_input, image_input], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz56A6bsYCQj",
        "outputId": "b56cede9-48b3-4452-b71f-0ae9a9de0f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.6820, Test Accuracy: 0.5714, Epochs: 11\n",
            "Fold 2 - Test Loss: 0.5345, Test Accuracy: 0.7857, Epochs: 18\n",
            "Fold 3 - Test Loss: 0.6047, Test Accuracy: 0.7143, Epochs: 15\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6905, Loss: 0.6071\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'three_modal.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Categorical Data ----------------------------\n",
        "    train_categorical = X_categorical.iloc[train_index]\n",
        "    test_categorical = X_categorical.iloc[test_index]\n",
        "\n",
        "    train_categorical, test_categorical = preprocess_categorical(train_categorical.copy(), test_categorical.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Image Data ----------------------------\n",
        "    train_image = []\n",
        "    for i in train_index:\n",
        "        train_image.append(X_image[i])\n",
        "\n",
        "    test_image = []\n",
        "    for i in test_index:\n",
        "        test_image.append(X_image[i])\n",
        "\n",
        "    train_image, test_image = preprocess_image(train_image.copy(), test_image.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_three_modal()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_categorical, train_image], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_categorical, test_image], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_categorical, test_image], test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQdlb15LsAWW"
      },
      "source": [
        "# Model Training: Exclusively Eye Tracking üëÅÔ∏è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hn_rz0KGANN"
      },
      "outputs": [],
      "source": [
        "backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zipxi8zBsBQw",
        "outputId": "4cec785e-58d1-4a34-fcee-570e1348af64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_timeseries: 42, X_categorical: 42, X_image: 42, y_actual: 42\n"
          ]
        }
      ],
      "source": [
        "X_timeseries = read_X_timeseries(MERGED_DATA, exclude_biomarkers=True)\n",
        "X_categorical = read_X_categorical(MERGED_DATA)\n",
        "X_image = read_X_image(FPOG_SCANPATHS_PATH)\n",
        "y = read_y_labels(MERGED_DATA)\n",
        "\n",
        "print(f\"X_timeseries: {len(X_timeseries)}, X_categorical: {len(X_categorical)}, X_image: {len(X_image)}, y_actual: {len(y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvCMVu6JACe9"
      },
      "source": [
        "## Single Modal: Timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cus9gxb_9yw",
        "outputId": "e1e0b6aa-00f3-4c32-b60c-edda30b68214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 (28) - Test Loss: 0.6927, Test Accuracy: 0.5714, Epochs: 11\n",
            "Fold 2 (28) - Test Loss: 0.4786, Test Accuracy: 0.8571, Epochs: 41\n",
            "Fold 3 (28) - Test Loss: 0.5924, Test Accuracy: 0.6429, Epochs: 41\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6905, Loss: 0.5879\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'single_modal_ex_biomarkers.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_single_modal()\n",
        "    history = model.fit(\n",
        "        train_time_series,\n",
        "        train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=(test_time_series, test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores = model.evaluate(test_time_series, test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} ({len(train_labels)}) - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDLPCtvBAHYe"
      },
      "source": [
        "## 2-Modal: Timeseries + Questionnaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9imPPfxATY_",
        "outputId": "dd06410d-f0b6-4fea-c9db-823dc42c85a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.6842, Test Accuracy: 0.7143, Epochs: 19\n",
            "Fold 2 - Test Loss: 0.6294, Test Accuracy: 0.7143, Epochs: 27\n",
            "Fold 3 - Test Loss: 0.6865, Test Accuracy: 0.6429, Epochs: 16\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6905, Loss: 0.6667\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'two_modal_ex_biomarkers.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Categorical Data ----------------------------\n",
        "    train_categorical = X_categorical.iloc[train_index]\n",
        "    test_categorical = X_categorical.iloc[test_index]\n",
        "\n",
        "    train_categorical, test_categorical = preprocess_categorical(train_categorical.copy(), test_categorical.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_two_modal_ts_q()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_categorical], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_categorical], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_categorical], test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO3hSbWMHgM5"
      },
      "source": [
        "## 2-Modal: Timeseries + Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKcTkUvcBZ46",
        "outputId": "870a9e55-5d8f-440f-d86e-cbe7b484dd10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.5654, Test Accuracy: 0.7857, Epochs: 19\n",
            "Fold 2 - Test Loss: 0.6416, Test Accuracy: 0.7857, Epochs: 15\n",
            "Fold 3 - Test Loss: 0.5411, Test Accuracy: 0.8571, Epochs: 16\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.8095, Loss: 0.5827\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'two_modal2_ex_biomarkers.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Image Data ----------------------------\n",
        "    train_image = []\n",
        "    for i in train_index:\n",
        "        train_image.append(X_image[i])\n",
        "\n",
        "    test_image = []\n",
        "    for i in test_index:\n",
        "        test_image.append(X_image[i])\n",
        "\n",
        "    train_image, test_image = preprocess_image(train_image.copy(), test_image.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_two_modal_ts_img()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_image], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_image], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_image], test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZSx2O3ZHmRp"
      },
      "source": [
        "## 3-Modal: Timeseries + Questionnaires + Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh-ai3gcHmgx",
        "outputId": "12a9c1f8-b7cd-4560-b5ff-06a2e605183f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Test Loss: 0.6731, Test Accuracy: 0.6429, Epochs: 12\n",
            "Fold 2 - Test Loss: 1.1288, Test Accuracy: 0.8571, Epochs: 16\n",
            "Fold 3 - Test Loss: 0.6965, Test Accuracy: 0.5714, Epochs: 13\n",
            "===============================================\n",
            "Average scores for all folds:\n",
            "Accuracy: 0.6905, Loss: 0.8328\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True) # Stratified K-Fold Cross Validation\n",
        "path = os.path.join(saved_models, 'three_modal_ex_biomarkers.keras')\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "num_datapoints = len(X_timeseries)\n",
        "\n",
        "for fold_no, (train_index, test_index) in enumerate(skf.split(X_timeseries, y)):\n",
        "    # ---------------------------- Preprocess Time Series Data ----------------------------\n",
        "    train_time_series = []\n",
        "    for i in train_index:\n",
        "        train_time_series.append(X_timeseries[i])\n",
        "\n",
        "    test_time_series = []\n",
        "    for i in test_index:\n",
        "        test_time_series.append(X_timeseries[i])\n",
        "\n",
        "    train_time_series, test_time_series = preprocess_timeseries(train_time_series.copy(), test_time_series.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Categorical Data ----------------------------\n",
        "    train_categorical = X_categorical.iloc[train_index]\n",
        "    test_categorical = X_categorical.iloc[test_index]\n",
        "\n",
        "    train_categorical, test_categorical = preprocess_categorical(train_categorical.copy(), test_categorical.copy())\n",
        "\n",
        "    # ---------------------------- Preprocess Image Data ----------------------------\n",
        "    train_image = []\n",
        "    for i in train_index:\n",
        "        train_image.append(X_image[i])\n",
        "\n",
        "    test_image = []\n",
        "    for i in test_index:\n",
        "        test_image.append(X_image[i])\n",
        "\n",
        "    train_image, test_image = preprocess_image(train_image.copy(), test_image.copy())\n",
        "\n",
        "    # ---------------------------- Get Labels ----------------------------\n",
        "    train_labels = y[train_index]\n",
        "    test_labels = y[test_index]\n",
        "\n",
        "    # ---------------------------- Build and Train the Model ----------------------------\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model = build_model_three_modal()\n",
        "    # Train the Model\n",
        "    history = model.fit(\n",
        "        [train_time_series, train_categorical, train_image], train_labels,\n",
        "        epochs=500,\n",
        "        batch_size=8,\n",
        "        validation_data=([test_time_series, test_categorical, test_image], test_labels),\n",
        "        callbacks=[early_stop, checkpoint],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate on the test fold\n",
        "    model = load_model(path)\n",
        "    scores =  model.evaluate([test_time_series, test_categorical, test_image], test_labels, verbose=0)\n",
        "    num_epochs_ran = len(history.history['loss'])\n",
        "    print(f'Fold {fold_no + 1} - Test Loss: {scores[0]:.4f}, Test Accuracy: {scores[1]:.4f}, Epochs: {num_epochs_ran}')\n",
        "    acc_per_fold.append(scores[1])\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "print('===============================================')\n",
        "print('Average scores for all folds:')\n",
        "print(f'Accuracy: {np.mean(acc_per_fold):.4f}, Loss: {np.mean(loss_per_fold):.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}