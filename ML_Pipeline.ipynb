{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJjGaP67HwET"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Wk0xkXoNjE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade h5py\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install imblearn\n",
        "!pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BJUx8CAw9kV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Stabilized Gaze Analysis System - Main Script\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import shutil\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "import time\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except ImportError:\n",
        "    drive = None\n",
        "\n",
        "class LogCapture:\n",
        "    def __init__(self, log_file_path: Path) -> None:\n",
        "        self.log_file_path = log_file_path\n",
        "        self.logger = None\n",
        "        self.console_handler = None\n",
        "        self.file_handler = None\n",
        "\n",
        "    def setup_logging(self) -> None:\n",
        "        self.logger = logging.getLogger('GazeAnalysisSystem')\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        self.file_handler = logging.FileHandler(str(self.log_file_path))\n",
        "        self.file_handler.setLevel(logging.INFO)\n",
        "        self.file_handler.setFormatter(formatter)\n",
        "        self.console_handler = logging.StreamHandler(sys.stdout)\n",
        "        self.console_handler.setLevel(logging.INFO)\n",
        "        self.console_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(self.file_handler)\n",
        "        self.logger.addHandler(self.console_handler)\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        if self.logger:\n",
        "            self.logger.removeHandler(self.file_handler)\n",
        "            self.logger.removeHandler(self.console_handler)\n",
        "            self.file_handler.close()\n",
        "            self.console_handler.close()\n",
        "\n",
        "class GradientDebugCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Custom callback to monitor gradient values during training.\"\"\"\n",
        "    def __init__(self, logger):\n",
        "        super().__init__()\n",
        "        self.logger = logger\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        try:\n",
        "            # Get trainable weights\n",
        "            weights = self.model.trainable_weights\n",
        "\n",
        "            # Create gradient tape context\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Get a batch from the training data\n",
        "                for x, y in logs.get('dataset', []):\n",
        "                    # ADD THIS DATA INSPECTION BLOCK\n",
        "                    self.logger.info(f\"Batch {batch}: Input data (x) stats:\")\n",
        "                    self.logger.info(f\"  Min: {tf.reduce_min(x).numpy()}\")\n",
        "                    self.logger.info(f\"  Max: {tf.reduce_max(x).numpy()}\")\n",
        "                    self.logger.info(f\"  Mean: {tf.reduce_mean(x).numpy()}\")\n",
        "                    self.logger.info(f\"  Std: {tf.math.reduce_std(x).numpy()}\")\n",
        "                    self.logger.info(f\"Batch {batch}: Target data (y) stats:\")\n",
        "                    self.logger.info(f\"  Min: {tf.reduce_min(y).numpy()}\")\n",
        "                    self.logger.info(f\"  Max: {tf.reduce_max(y).numpy()}\")\n",
        "                    self.logger.info(f\"  Mean: {tf.reduce_mean(y).numpy()}\")\n",
        "                    self.logger.info(f\"  Std: {tf.math.reduce_std(y).numpy()}\")\n",
        "                    if tf.reduce_any(tf.math.is_nan(x)):\n",
        "                        self.logger.warning(f\"Batch {batch}: NaN in input data (x)\")\n",
        "                    if tf.reduce_any(tf.math.is_inf(x)):\n",
        "                        self.logger.warning(f\"Batch {batch}: Inf in input data (x)\")\n",
        "                    if tf.reduce_any(tf.math.is_nan(y)):\n",
        "                        self.logger.warning(f\"Batch {batch}: NaN in target data (y)\")\n",
        "                    if tf.reduce_any(tf.math.is_inf(y)):\n",
        "                        self.logger.warning(f\"Batch {batch}: Inf in target data (y)\")\n",
        "\n",
        "                    predictions = self.model(x, training=True)\n",
        "                    loss = self.model.compiled_loss(y, predictions)\n",
        "                    break  # Just use first batch\n",
        "\n",
        "            # Calculate gradients\n",
        "            if hasattr(tape, 'gradient'):\n",
        "                grads = tape.gradient(loss, weights)\n",
        "\n",
        "                # Check for NaN/Inf values\n",
        "                has_nan = any(tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None)\n",
        "                has_inf = any(tf.reduce_any(tf.math.is_inf(g)) for g in grads if g is not None)\n",
        "\n",
        "                if has_nan or has_inf:\n",
        "                    self.logger.warning(f\"Batch {batch}: NaN/Inf gradients detected\")\n",
        "                    for i, (w, g) in enumerate(zip(weights, grads)):\n",
        "                        if g is not None and (tf.reduce_any(tf.math.is_nan(g)) or tf.reduce_any(tf.math.is_inf(g))):\n",
        "                            self.logger.warning(f\"Layer {i}: {w.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log but don't stop training\n",
        "            self.logger.warning(f\"Error in gradient monitoring: {str(e)}\")\n",
        "            pass\n",
        "\n",
        "class NanLossCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Callback to monitor for NaN losses.\"\"\"\n",
        "    def __init__(self, logger):\n",
        "        super().__init__()\n",
        "        self.logger = logger\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if 'loss' in logs and (np.isnan(logs['loss']) or np.isinf(logs['loss'])):\n",
        "            self.logger.warning(f\"NaN/Inf loss detected at batch {batch}: {logs['loss']}\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class GazeAnalysisSystem:\n",
        "    def __init__(\n",
        "        self,\n",
        "        gaze_data_path: str = '/content/drive/MyDrive/pilotdata',\n",
        "        scores_data_path: str = '/content/drive/MyDrive/output',\n",
        "        window_size: int = 64,\n",
        "        stride: int = 1,\n",
        "        model_type: str = \"cnn_transformer\",\n",
        "        batch_size: int = 64,\n",
        "        use_robust_scaler: bool = True,\n",
        "        clip_range: tuple = (-3, 3),\n",
        "        max_seq_per_participant: int = 50000\n",
        "    ) -> None:\n",
        "        # Create a run folder with timestamp\n",
        "        run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.run_folder = Path(scores_data_path) / f'run_{run_timestamp}'\n",
        "        self.run_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Setup logging\n",
        "        log_dir = self.run_folder / 'logs'\n",
        "        log_dir.mkdir(exist_ok=True)\n",
        "        self.log_file_path = log_dir / f'gaze_analysis_{run_timestamp}.log'\n",
        "        self.log_capture = LogCapture(self.log_file_path)\n",
        "        self.log_capture.setup_logging()\n",
        "        self.logger = logging.getLogger('GazeAnalysisSystem')\n",
        "        self.logger.info(\"Initializing GazeAnalysisSystem\")\n",
        "\n",
        "        # Mount drive if in Colab\n",
        "        if drive and not Path('/content/drive/MyDrive').exists():\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Store configuration\n",
        "        self.gaze_data_path = Path(gaze_data_path)\n",
        "        self.scores_data_path = Path(scores_data_path)\n",
        "        self.sequence_length = window_size\n",
        "        self.stride = stride\n",
        "        self.batch_size = batch_size\n",
        "        self.model_type = model_type.lower()\n",
        "        self.use_robust_scaler = use_robust_scaler\n",
        "        self.clip_range = clip_range\n",
        "        self.max_seq_per_participant = max_seq_per_participant\n",
        "\n",
        "        # Define features and configuration\n",
        "        self.features = [\n",
        "            'FPOGX', 'FPOGY', 'FPOGD',\n",
        "            'BPOGX', 'BPOGY',\n",
        "            'LPCX', 'LPCY', 'LPD', 'LPS',\n",
        "            'RPCX', 'RPCY', 'RPD', 'RPS',\n",
        "            'LPMM', 'RPMM',\n",
        "            'CX', 'CY'\n",
        "        ]\n",
        "        self.selected_features = self.features\n",
        "        self.target = 'Row_Total_Score'\n",
        "        self.timestamp_column = 'TIMESTAMP'\n",
        "\n",
        "        # Initialize scalers\n",
        "        self.scaler = None\n",
        "        self.target_scaler = MinMaxScaler()\n",
        "\n",
        "        self.logger.info(\"Initialization complete\")\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'log_capture'):\n",
        "            self.log_capture.cleanup()\n",
        "\n",
        "    def get_participant_ids(self) -> list:\n",
        "        \"\"\"Extract participant IDs from gaze data filenames.\"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Getting participant IDs\")\n",
        "            all_files = os.listdir(self.gaze_data_path)\n",
        "            gaze_files = [f for f in all_files if f.endswith('_all_gaze.csv')]\n",
        "            if not gaze_files:\n",
        "                raise ValueError(f\"No '_all_gaze.csv' files found in {self.gaze_data_path}\")\n",
        "            participant_ids = [f.split('_')[0] for f in gaze_files]\n",
        "            self.logger.info(f\"Found {len(participant_ids)} participants: {participant_ids}\")\n",
        "            return participant_ids\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error getting participant IDs\")\n",
        "            raise\n",
        "\n",
        "    def validate_and_clean_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Validate and clean data by handling outliers and invalid values.\"\"\"\n",
        "        try:\n",
        "            self.logger.info(\"Starting data validation and cleaning\")\n",
        "\n",
        "            # Check for infinite or NaN values\n",
        "            for feature in self.features:\n",
        "                inf_mask = np.isinf(data[feature])\n",
        "                nan_mask = np.isnan(data[feature])\n",
        "                if inf_mask.any():\n",
        "                    self.logger.warning(f\"Infinite values found in {feature}: {inf_mask.sum()} values\")\n",
        "                    data.loc[inf_mask, feature] = data[feature].median()\n",
        "                if nan_mask.any():\n",
        "                    self.logger.warning(f\"NaN values found in {feature}: {nan_mask.sum()} values\")\n",
        "                    data.loc[nan_mask, feature] = data[feature].median()\n",
        "\n",
        "            # Handle outliers using IQR method\n",
        "            for feature in self.features:\n",
        "                q1 = data[feature].quantile(0.25)\n",
        "                q3 = data[feature].quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                lower_bound = q1 - 3 * iqr\n",
        "                upper_bound = q3 + 3 * iqr\n",
        "                outliers = (data[feature] < lower_bound) | (data[feature] > upper_bound)\n",
        "                if outliers.any():\n",
        "                    self.logger.warning(f\"Outliers found in {feature}: {outliers.sum()} values\")\n",
        "                    data.loc[outliers, feature] = data[feature].median()\n",
        "\n",
        "            # Log statistics after cleaning\n",
        "            for feature in self.features:\n",
        "                stats = data[feature].describe()\n",
        "                self.logger.info(f\"Statistics for {feature}:\")\n",
        "                self.logger.info(f\"  Mean: {stats['mean']:.3f}\")\n",
        "                self.logger.info(f\"  Std: {stats['std']:.3f}\")\n",
        "                self.logger.info(f\"  Min: {stats['min']:.3f}\")\n",
        "                self.logger.info(f\"  Max: {stats['max']:.3f}\")\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error in validate_and_clean_data\")\n",
        "            raise\n",
        "\n",
        "    def align_data_at_intercept(self, df_gaze: pd.DataFrame, df_scores: pd.DataFrame):\n",
        "        \"\"\"Align gaze and score data using an intercept time based on score phases.\"\"\"\n",
        "        intercept_time = 0\n",
        "        if 'Phase' in df_scores.columns and 'Time' in df_scores.columns:\n",
        "            final_rows = df_scores[df_scores['Phase'] == 'Final']\n",
        "            if not final_rows.empty:\n",
        "                intercept_time = final_rows.iloc[0]['Time']\n",
        "            else:\n",
        "                stepdown = df_scores[df_scores['Phase'] == 'Stepdown']\n",
        "                if not stepdown.empty:\n",
        "                    intercept_time = stepdown.iloc[0]['Time']\n",
        "\n",
        "        df_scores = df_scores.copy()\n",
        "        df_scores['RelTime'] = df_scores['Time'] - intercept_time\n",
        "        df_scores = df_scores[df_scores['RelTime'] >= 0].reset_index(drop=True)\n",
        "\n",
        "        df_gaze = df_gaze.copy()\n",
        "        df_gaze['RelTime'] = df_gaze[self.timestamp_column] - intercept_time\n",
        "        df_gaze = df_gaze[df_gaze['RelTime'] >= 0].reset_index(drop=True)\n",
        "\n",
        "        df_scores.sort_values('RelTime', inplace=True)\n",
        "        df_gaze.sort_values('RelTime', inplace=True)\n",
        "        return df_gaze, df_scores, intercept_time\n",
        "\n",
        "    def load_and_preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load and preprocess gaze and score data for all participants with improved stability.\"\"\"\n",
        "        try:\n",
        "            participant_ids = self.get_participant_ids()\n",
        "            all_data = []\n",
        "            processed = 0\n",
        "\n",
        "            for pid in participant_ids:\n",
        "                self.logger.info(f\"Processing participant: {pid}\")\n",
        "                gaze_file = self.gaze_data_path / f\"{pid}_all_gaze.csv\"\n",
        "                if not gaze_file.exists():\n",
        "                    self.logger.warning(f\"No gaze file for {pid}, skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # Load and validate gaze data\n",
        "                try:\n",
        "                    df_gaze = pd.read_csv(gaze_file)\n",
        "                    self.logger.info(f\"Loaded gaze data for {pid}: {df_gaze.shape}\")\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error reading gaze file for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Handle time column\n",
        "                time_cols = [c for c in df_gaze.columns if c.startswith(\"TIME(\")]\n",
        "                if not time_cols:\n",
        "                    self.logger.warning(f\"No TIME(...) column for {pid}, skipping...\")\n",
        "                    continue\n",
        "                time_col = time_cols[0]\n",
        "                df_gaze.rename(columns={time_col: 'TimeRaw'}, inplace=True)\n",
        "\n",
        "                # Create relative timestamp and validate\n",
        "                try:\n",
        "                    df_gaze['TIMESTAMP'] = df_gaze['TimeRaw'] - df_gaze['TimeRaw'].iloc[0]\n",
        "                    if df_gaze['TIMESTAMP'].isna().any():\n",
        "                        self.logger.warning(f\"NaN timestamps found for {pid}, fixing...\")\n",
        "                        df_gaze['TIMESTAMP'] = df_gaze['TIMESTAMP'].interpolate(method='linear')\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing timestamps for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Load and validate scores data\n",
        "                scores_file = self.scores_data_path / pid / f\"{pid}_detailed_scores.csv\"\n",
        "                if not scores_file.exists():\n",
        "                    self.logger.warning(f\"No detailed scores for {pid}, skipping...\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    df_scores = pd.read_csv(scores_file)\n",
        "                    self.logger.info(f\"Loaded scores data for {pid}: {df_scores.shape}\")\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error reading scores file for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Process scores timestamps\n",
        "                try:\n",
        "                    df_scores['Time'] = df_scores['Time'] - df_scores['Time'].iloc[0]\n",
        "                    df_scores['TIMESTAMP'] = df_scores['Time']\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing score timestamps for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Align data and validate\n",
        "                try:\n",
        "                    df_gaze_align, df_scores_align, intercept = self.align_data_at_intercept(df_gaze, df_scores)\n",
        "                    self.logger.info(f\"Aligned data at intercept time: {intercept}\")\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error aligning data for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Prepare aligned data\n",
        "                df_gaze_align.drop(columns=['TIMESTAMP'], inplace=True, errors='ignore')\n",
        "                df_scores_align.drop(columns=['TIMESTAMP'], inplace=True, errors='ignore')\n",
        "                df_gaze_align.rename(columns={'RelTime': 'TIMESTAMP'}, inplace=True)\n",
        "                df_scores_align.rename(columns={'RelTime': 'TIMESTAMP'}, inplace=True)\n",
        "                df_gaze_align.sort_values('TIMESTAMP', inplace=True)\n",
        "                df_scores_align.sort_values('TIMESTAMP', inplace=True)\n",
        "\n",
        "                # Merge data with validation\n",
        "                try:\n",
        "                    df_merged = pd.merge_asof(\n",
        "                        df_gaze_align,\n",
        "                        df_scores_align[['TIMESTAMP', 'Row_Total_Score']],\n",
        "                        on='TIMESTAMP',\n",
        "                        direction='nearest',\n",
        "                        tolerance=1.0\n",
        "                    )\n",
        "\n",
        "                    # Validate merge results\n",
        "                    if df_merged.empty:\n",
        "                        self.logger.warning(f\"Empty merged data for {pid}, skipping...\")\n",
        "                        continue\n",
        "\n",
        "                    missing_score = df_merged['Row_Total_Score'].isna().sum()\n",
        "                    if missing_score > 0:\n",
        "                        self.logger.warning(f\"Missing scores in merged data for {pid}: {missing_score} rows\")\n",
        "                        df_merged.dropna(subset=['Row_Total_Score'], inplace=True)\n",
        "\n",
        "                    # Handle missing feature values\n",
        "                    for feature in self.features:\n",
        "                        missing_count = df_merged[feature].isna().sum()\n",
        "                        if missing_count > 0:\n",
        "                            self.logger.warning(f\"Missing values in {feature} for {pid}: {missing_count} rows\")\n",
        "                            df_merged[feature] = df_merged[feature].interpolate(method='linear')\n",
        "\n",
        "                    # Final validation\n",
        "                    if df_merged[self.features].isna().any().any():\n",
        "                        self.logger.warning(f\"Remaining NaN values for {pid}, using ffill/bfill\")\n",
        "                        df_merged[self.features] = df_merged[self.features].ffill().bfill()\n",
        "\n",
        "                    df_merged['participant_id'] = pid\n",
        "\n",
        "                    # Log statistics for this participant\n",
        "                    self.logger.info(f\"Processed data for {pid}:\")\n",
        "                    self.logger.info(f\"  Final shape: {df_merged.shape}\")\n",
        "                    self.logger.info(f\"  Score range: {df_merged['Row_Total_Score'].min():.2f} to {df_merged['Row_Total_Score'].max():.2f}\")\n",
        "\n",
        "                    all_data.append(df_merged)\n",
        "                    processed += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error merging data for {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if processed == 0:\n",
        "                raise ValueError(\"No participants processed successfully\")\n",
        "\n",
        "            # Combine all data\n",
        "            data = pd.concat(all_data, ignore_index=True)\n",
        "            self.logger.info(f\"Combined data shape: {data.shape}\")\n",
        "\n",
        "            # Validate and clean combined data\n",
        "            data = self.validate_and_clean_data(data)\n",
        "\n",
        "            # Scale features\n",
        "            if self.use_robust_scaler:\n",
        "                self.scaler = RobustScaler()\n",
        "                self.logger.info(\"Using RobustScaler for feature scaling\")\n",
        "\n",
        "\n",
        "                for feature in self.features:\n",
        "                    q1 = data[feature].quantile(0.25)\n",
        "                    q3 = data[feature].quantile(0.75)\n",
        "                    iqr = q3 - q1\n",
        "                    if iqr == 0:\n",
        "                        self.logger.warning(f\"Feature '{feature}' has zero IQR.  This might cause issues with RobustScaler.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                self.scaler = StandardScaler()\n",
        "                self.logger.info(\"Using StandardScaler for feature scaling\")\n",
        "\n",
        "\n",
        "            # Scale features with validation\n",
        "            try:\n",
        "                scaled_features = self.scaler.fit_transform(data[self.features])\n",
        "                clip_low, clip_high = self.clip_range\n",
        "                scaled_features = np.clip(scaled_features, clip_low, clip_high)\n",
        "\n",
        "                for i, feature in enumerate(self.features):\n",
        "                    data[feature] = scaled_features[:, i]\n",
        "\n",
        "                # Verify scaling results\n",
        "                for feature in self.features:\n",
        "                    stats = data[feature].describe()\n",
        "                    self.logger.info(f\"Scaled {feature}:\")\n",
        "                    self.logger.info(f\"  Mean: {stats['mean']:.3f}\")\n",
        "                    self.logger.info(f\"  Std: {stats['std']:.3f}\")\n",
        "                    self.logger.info(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
        "                    # Check for any remaining NaN or inf values.\n",
        "                    if data[feature].isnull().any():\n",
        "                        self.logger.error(f\"NaN values found in {feature} after scaling!\")\n",
        "                    if np.isinf(data[feature]).any():\n",
        "                        self.logger.error(f\"Inf values found in {feature} after scaling!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error scaling features: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Scale target variable\n",
        "            try:\n",
        "                target_values = data[self.target].values.reshape(-1, 1)\n",
        "                data[self.target] = self.target_scaler.fit_transform(target_values)\n",
        "\n",
        "                # Verify target scaling\n",
        "                stats = data[self.target].describe()\n",
        "                self.logger.info(f\"Scaled target variable:\")\n",
        "                self.logger.info(f\"  Mean: {stats['mean']:.3f}\")\n",
        "                self.logger.info(f\"  Std: {stats['std']:.3f}\")\n",
        "                self.logger.info(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error scaling target variable: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Save scalers\n",
        "            try:\n",
        "                scaler_path = self.run_folder / 'feature_scaler.gz'\n",
        "                target_scaler_path = self.run_folder / 'target_scaler.gz'\n",
        "                joblib.dump(self.scaler, scaler_path)\n",
        "                joblib.dump(self.target_scaler, target_scaler_path)\n",
        "                self.logger.info(f\"Saved scalers to {self.run_folder}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error saving scalers: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            self.data = data\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Critical error in load_and_preprocess_data\")\n",
        "            raise\n",
        "\n",
        "    def build_model(self, sequence_length: int, n_features: int) -> tf.keras.Model:\n",
        "        \"\"\"Build model with balanced regularization.\"\"\"\n",
        "        try:\n",
        "            # Define a moderate regularizer\n",
        "            regularizer = tf.keras.regularizers.L2(l2=1e-5)  # Reduced from 1e-4\n",
        "\n",
        "            # Define optimizer\n",
        "            optimizer = tf.keras.optimizers.Adam(\n",
        "                learning_rate=1e-4,\n",
        "                clipnorm=0.5\n",
        "            )\n",
        "\n",
        "            if self.model_type == \"cnn\":\n",
        "                inputs = tf.keras.layers.Input(shape=(sequence_length, n_features))\n",
        "\n",
        "\n",
        "                x = tf.keras.layers.BatchNormalization()(inputs)\n",
        "\n",
        "                # First conv block\n",
        "                conv1 = tf.keras.layers.Conv1D(\n",
        "                    32, kernel_size=3, padding=\"same\",\n",
        "                    kernel_regularizer=regularizer,\n",
        "                    kernel_initializer='he_normal'\n",
        "                )(x)\n",
        "                conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
        "                conv1 = tf.keras.layers.Activation(\"relu\")(conv1)\n",
        "                conv1 = tf.keras.layers.Dropout(0.1)(conv1)\n",
        "\n",
        "                # Second conv block\n",
        "                conv2 = tf.keras.layers.Conv1D(\n",
        "                    64, kernel_size=3, padding=\"same\",\n",
        "                    kernel_regularizer=regularizer,\n",
        "                    kernel_initializer='he_normal'\n",
        "                )(conv1)\n",
        "                conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
        "                conv2 = tf.keras.layers.Activation(\"relu\")(conv2)\n",
        "                conv2 = tf.keras.layers.Dropout(0.1)(conv2)\n",
        "\n",
        "                # Global pooling and final dense layers\n",
        "                x = tf.keras.layers.GlobalAveragePooling1D()(conv2)\n",
        "                x = tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizer)(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "            elif self.model_type == \"lstm\":\n",
        "                inputs = tf.keras.layers.Input(shape=(sequence_length, n_features))\n",
        "                x = tf.keras.layers.BatchNormalization()(inputs)\n",
        "\n",
        "                x = tf.keras.layers.LSTM(\n",
        "                    32, return_sequences=True,\n",
        "                    kernel_regularizer=regularizer,\n",
        "                    recurrent_regularizer=regularizer,\n",
        "                    recurrent_dropout=0.0\n",
        "                )(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "                x = tf.keras.layers.LSTM(\n",
        "                    16,\n",
        "                    kernel_regularizer=regularizer,\n",
        "                    recurrent_regularizer=regularizer,\n",
        "                    recurrent_dropout=0.0\n",
        "                )(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                x = tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizer)(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "            elif self.model_type == \"cnn_transformer\":\n",
        "                inputs = tf.keras.layers.Input(shape=(sequence_length, n_features))\n",
        "                x = tf.keras.layers.BatchNormalization()(inputs)\n",
        "\n",
        "                # Initial projection\n",
        "                x = tf.keras.layers.Dense(\n",
        "                    32,\n",
        "                    kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "                    bias_initializer='zeros',\n",
        "                    kernel_regularizer=regularizer\n",
        "                )(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "                # Convolutional feature extraction\n",
        "                for filters in [32, 64]:\n",
        "                    x = tf.keras.layers.Conv1D(\n",
        "                        filters=filters,\n",
        "                        kernel_size=3,\n",
        "                        padding=\"same\",\n",
        "                        kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "                        bias_initializer='zeros',\n",
        "                        kernel_regularizer=regularizer\n",
        "                    )(x)\n",
        "                    x = tf.keras.layers.BatchNormalization()(x)\n",
        "                    x = tf.keras.layers.Activation('relu')(x)\n",
        "                    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "                # Transformer block with scaled attention\n",
        "                attention_output = tf.keras.layers.MultiHeadAttention(\n",
        "                    key_dim=32,\n",
        "                    num_heads=4,\n",
        "                    dropout=0.1,\n",
        "                    kernel_initializer=tf.keras.initializers.HeNormal()\n",
        "                )(x, x)\n",
        "                x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output * 0.5)\n",
        "\n",
        "                # Feed-forward network with residual connection\n",
        "                ffn = tf.keras.layers.Dense(\n",
        "                    64,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "                    bias_initializer='zeros',\n",
        "                    kernel_regularizer=regularizer\n",
        "                )(x)\n",
        "                ffn = tf.keras.layers.BatchNormalization()(ffn)\n",
        "                ffn = tf.keras.layers.Dropout(0.1)(ffn)\n",
        "                x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ffn * 0.5)\n",
        "\n",
        "                # Output layers\n",
        "                x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                x = tf.keras.layers.Dense(\n",
        "                    32,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "                    bias_initializer='zeros',\n",
        "                    kernel_regularizer=regularizer\n",
        "                )(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                x = tf.keras.layers.Dropout(0.1)(x)\n",
        "                outputs = tf.keras.layers.Dense(\n",
        "                    1,\n",
        "                    activation='linear',\n",
        "                    kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "                    bias_initializer='zeros'\n",
        "                )(x)\n",
        "\n",
        "            else:\n",
        "                self.logger.error(f\"Unknown model_type '{self.model_type}'. Defaulting to CNN.\")\n",
        "                return self.build_model(sequence_length, n_features)\n",
        "\n",
        "            model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "            # Gradient monitoring callback\n",
        "            self.additional_callbacks = [\n",
        "                GradientDebugCallback(self.logger),\n",
        "                NanLossCallback(self.logger)\n",
        "            ]\n",
        "\n",
        "            # Compile with stable configuration\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=[\n",
        "                    tf.keras.metrics.MeanAbsoluteError(),\n",
        "                    tf.keras.metrics.MeanSquaredError()\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.logger.info(f\"Built stabilized {self.model_type.upper()} model with input shape ({sequence_length}, {n_features})\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error building model\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def create_training_config(self, train_dataset):\n",
        "        \"\"\"Create a stable training configuration with improved monitoring.\"\"\"\n",
        "        callbacks = [\n",
        "            # Early stopping with restored weights\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=3,\n",
        "                verbose=1,\n",
        "                min_lr=1e-7\n",
        "            ),\n",
        "\n",
        "            # Model checkpoint\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=str(self.run_folder / 'checkpoints' / 'model_{epoch:02d}.weights.h5'),\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Simple loss monitoring\n",
        "            NanLossCallback(self.logger),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir=str(self.run_folder / 'tensorboard_logs'), histogram_freq=1)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "    def evaluate_participant_generalization(self):\n",
        "        \"\"\"Evaluate how well the model generalizes to entirely new participants.\"\"\"\n",
        "        try:\n",
        "            participant_ids = self.data['participant_id'].unique()\n",
        "            n_participants = len(participant_ids)\n",
        "\n",
        "            self.logger.info(f\"Performing leave-one-participant-out evaluation with {n_participants} participants\")\n",
        "\n",
        "            # Prepare results tracking\n",
        "            participant_metrics = []\n",
        "\n",
        "            for test_pid in participant_ids:\n",
        "                self.logger.info(f\"Using participant {test_pid} as test set\")\n",
        "\n",
        "                # Split data\n",
        "                train_mask = self.data['participant_id'] != test_pid\n",
        "                test_mask = self.data['participant_id'] == test_pid\n",
        "\n",
        "                # Create sequences for training data\n",
        "                train_data = self.data[train_mask]\n",
        "                X_train, y_train = self._create_sequences(train_data)\n",
        "\n",
        "                # Create sequences for test participant\n",
        "                test_data = self.data[test_mask]\n",
        "                X_test, y_test = self._create_sequences(test_data)\n",
        "\n",
        "                # Scale data using only training data\n",
        "                if self.use_robust_scaler:\n",
        "                    scaler = RobustScaler().fit(X_train.reshape(-1, X_train.shape[-1]))\n",
        "                else:\n",
        "                    scaler = StandardScaler().fit(X_train.reshape(-1, X_train.shape[-1]))\n",
        "\n",
        "                target_scaler = MinMaxScaler().fit(y_train.reshape(-1, 1))\n",
        "\n",
        "                # Transform data\n",
        "                X_train_scaled = scaler.transform(X_train.reshape(-1, X_train.shape[-1]))\n",
        "                X_train_scaled = np.clip(X_train_scaled, self.clip_range[0], self.clip_range[1])\n",
        "                X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
        "\n",
        "                X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n",
        "                X_test_scaled = np.clip(X_test_scaled, self.clip_range[0], self.clip_range[1])\n",
        "                X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "                y_train_scaled = target_scaler.transform(y_train.reshape(-1, 1)).ravel()\n",
        "                y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "                # Build and train model\n",
        "                model = self.build_model(X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "                # Create datasets\n",
        "                train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train_scaled))\n",
        "                train_dataset = train_dataset.shuffle(10000).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "                test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_scaled))\n",
        "                test_dataset = test_dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "                # Train\n",
        "                model.fit(\n",
        "                    train_dataset,\n",
        "                    epochs=50,\n",
        "                    callbacks=self.create_training_config(train_dataset),\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                # Evaluate\n",
        "                test_results = model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "                # Record metrics\n",
        "                participant_metrics.append({\n",
        "                    'participant_id': test_pid,\n",
        "                    'test_loss': float(test_results[0]),\n",
        "                    'test_mae': float(test_results[1]),\n",
        "                    'test_mse': float(test_results[2])\n",
        "                })\n",
        "\n",
        "                # Clean up\n",
        "                tf.keras.backend.clear_session()\n",
        "\n",
        "            # Save and analyze results\n",
        "            metrics_df = pd.DataFrame(participant_metrics)\n",
        "            metrics_df.to_csv(self.run_folder / 'participant_generalization.csv', index=False)\n",
        "\n",
        "            # Log summary statistics\n",
        "            self.logger.info(\"Participant generalization results:\")\n",
        "            self.logger.info(f\"Average MAE: {metrics_df['test_mae'].mean():.4f}\")\n",
        "            self.logger.info(f\"Std Dev MAE: {metrics_df['test_mae'].std():.4f}\")\n",
        "            self.logger.info(f\"Min MAE: {metrics_df['test_mae'].min():.4f} (Participant {metrics_df.iloc[metrics_df['test_mae'].argmin()]['participant_id']})\")\n",
        "            self.logger.info(f\"Max MAE: {metrics_df['test_mae'].max():.4f} (Participant {metrics_df.iloc[metrics_df['test_mae'].argmax()]['participant_id']})\")\n",
        "\n",
        "            return metrics_df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error in participant generalization evaluation\")\n",
        "            raise\n",
        "    def _create_sequences(self, df):\n",
        "        \"\"\"Helper method to create sequences from a dataframe.\"\"\"\n",
        "        feature_data = df[self.features].values\n",
        "        n_samples = len(df)\n",
        "\n",
        "        if n_samples < self.sequence_length:\n",
        "            return np.array([]), np.array([])\n",
        "\n",
        "        n_sequences = (n_samples - self.sequence_length + self.stride) // self.stride\n",
        "        sequences = np.zeros((n_sequences, self.sequence_length, len(self.features)))\n",
        "        targets = np.zeros(n_sequences)\n",
        "\n",
        "        for i in range(n_sequences):\n",
        "            start_idx = i * self.stride\n",
        "            end_idx = start_idx + self.sequence_length\n",
        "            sequences[i] = feature_data[start_idx:end_idx]\n",
        "            targets[i] = df[self.target].values[end_idx - 1]\n",
        "\n",
        "        return sequences, targets\n",
        "    def train_and_evaluate(self) -> tf.keras.Model:\n",
        "        \"\"\"Train the model using cross-validation with GPU optimization.\"\"\"\n",
        "        try:\n",
        "            best_model = None\n",
        "            best_val_metric = float('inf')\n",
        "            best_fold = None\n",
        "            fold_metrics = []\n",
        "            histories = []\n",
        "\n",
        "            # Create sequences (keeping existing sequence creation code)\n",
        "            self.logger.info(\"Starting sequence creation...\")\n",
        "            participant_ids = self.data['participant_id'].unique()\n",
        "            X_list, y_list, timestamps_list, participant_ids_list = [], [], [], []\n",
        "\n",
        "            for pid in participant_ids:\n",
        "                try:\n",
        "                    self.logger.info(f\"Processing sequences for participant {pid}\")\n",
        "                    df_participant = self.data[self.data['participant_id'] == pid]\n",
        "\n",
        "                    # Calculate sequences\n",
        "                    feature_data = df_participant[self.features].values\n",
        "                    n_samples = len(df_participant)\n",
        "\n",
        "                    if n_samples < self.sequence_length:\n",
        "                        self.logger.warning(f\"Insufficient samples for participant {pid}\")\n",
        "                        continue\n",
        "\n",
        "                    n_sequences = (n_samples - self.sequence_length + self.stride) // self.stride\n",
        "                    sequences = np.zeros((n_sequences, self.sequence_length, len(self.features)))\n",
        "                    targets = np.zeros(n_sequences)\n",
        "                    timestamps = np.zeros(n_sequences)\n",
        "\n",
        "                    for i in range(n_sequences):\n",
        "                        start_idx = i * self.stride\n",
        "                        end_idx = start_idx + self.sequence_length\n",
        "                        sequences[i] = feature_data[start_idx:end_idx]\n",
        "                        targets[i] = df_participant[self.target].values[end_idx - 1]\n",
        "                        timestamps[i] = df_participant[self.timestamp_column].values[end_idx - 1]\n",
        "\n",
        "\n",
        "                    max_sequences = min(self.max_seq_per_participant, 40000)\n",
        "                    if n_sequences > max_sequences:\n",
        "                        indices = np.random.choice(n_sequences, max_sequences, replace=False)\n",
        "                        sequences = sequences[indices]\n",
        "                        targets = targets[indices]\n",
        "                        timestamps = timestamps[indices]\n",
        "                        n_sequences = max_sequences\n",
        "\n",
        "                    # Free memory explicitly\n",
        "                    del feature_data\n",
        "\n",
        "                    X_list.append(sequences)\n",
        "                    y_list.append(targets)\n",
        "                    timestamps_list.append(timestamps)\n",
        "                    participant_ids_list.append(np.full(len(targets), pid))\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.warning(f\"Error processing participant {pid}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not X_list:\n",
        "                raise ValueError(\"No sequences were created successfully\")\n",
        "\n",
        "            X = np.concatenate(X_list, axis=0)\n",
        "            y = np.concatenate(y_list, axis=0)\n",
        "            timestamps = np.concatenate(timestamps_list, axis=0)\n",
        "            participant_ids = np.concatenate(participant_ids_list, axis=0)\n",
        "\n",
        "            self.logger.info(f\"Created sequences with shape: {X.shape}\")\n",
        "            self.logger.info(f\"Target values range: {y.min():.3f} to {y.max():.3f}\")\n",
        "\n",
        "            # GPU Strategy setup\n",
        "            try:\n",
        "                self.logger.info(\"Setting up GPU strategy...\")\n",
        "                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "                tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "                physical_devices = tf.config.list_physical_devices('GPU')\n",
        "                if physical_devices:\n",
        "                    for device in physical_devices:\n",
        "                        tf.config.experimental.set_memory_growth(device, True)\n",
        "                    strategy = tf.distribute.MirroredStrategy()\n",
        "                    self.logger.info(f\"GPU Strategy initialized: {strategy.__class__.__name__}\")\n",
        "                else:\n",
        "                    strategy = tf.distribute.get_strategy()\n",
        "                    self.logger.warning(\"No GPU found, using default strategy\")\n",
        "\n",
        "                n_cores = strategy.num_replicas_in_sync\n",
        "                self.logger.info(f\"Number of devices: {n_cores}\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"GPU initialization failed: {str(e)}. Falling back to default strategy.\")\n",
        "                strategy = tf.distribute.get_strategy()\n",
        "                n_cores = 1\n",
        "\n",
        "            # Memory monitoring callback\n",
        "            class MemoryMonitorCallback(tf.keras.callbacks.Callback):\n",
        "                def __init__(self, logger):\n",
        "                    super().__init__()\n",
        "                    self.logger = logger\n",
        "\n",
        "                def on_epoch_end(self, epoch, logs=None):\n",
        "                    try:\n",
        "                        import psutil\n",
        "                        process = psutil.Process()\n",
        "                        memory_info = process.memory_info()\n",
        "                        self.logger.info(f\"Memory usage after epoch {epoch}: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Cross-validation\n",
        "            groups = participant_ids\n",
        "            gkf = GroupKFold(n_splits=3)\n",
        "            fold_idx = 0\n",
        "\n",
        "            for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
        "                fold_idx += 1\n",
        "                self.logger.info(f\"\\n{'='*50}\")\n",
        "                self.logger.info(f\"Training fold {fold_idx}/3\")\n",
        "                self.logger.info(f\"Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
        "\n",
        "                try:\n",
        "                    with strategy.scope():\n",
        "                        batch_size = min(128 * n_cores, 1024)\n",
        "                        batch_size = (batch_size // n_cores) * n_cores\n",
        "                        self.logger.info(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "                        # Get train/val split\n",
        "                        X_train, X_val = X[train_idx], X[val_idx]\n",
        "                        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                        # Create fold-specific scalers\n",
        "                        if self.use_robust_scaler:\n",
        "                            fold_scaler = RobustScaler()\n",
        "                        else:\n",
        "                            fold_scaler = StandardScaler()\n",
        "\n",
        "                        fold_target_scaler = MinMaxScaler()\n",
        "\n",
        "                        # Scale features for this fold only\n",
        "                        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
        "                        X_train_scaled = fold_scaler.fit_transform(X_train_reshaped)\n",
        "                        X_train_scaled = np.clip(X_train_scaled, self.clip_range[0], self.clip_range[1])\n",
        "                        X_train = X_train_scaled.reshape(X_train.shape)\n",
        "\n",
        "                        # Scale validation with training scaler\n",
        "                        X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
        "                        X_val_scaled = fold_scaler.transform(X_val_reshaped)\n",
        "                        X_val_scaled = np.clip(X_val_scaled, self.clip_range[0], self.clip_range[1])\n",
        "                        X_val = X_val_scaled.reshape(X_val.shape)\n",
        "\n",
        "                        # Scale targets\n",
        "                        y_train = fold_target_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "                        y_val = fold_target_scaler.transform(y_val.reshape(-1, 1)).ravel()\n",
        "\n",
        "                        # Create TensorFlow datasets\n",
        "                        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "                        train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
        "                        train_dataset = train_dataset.batch(batch_size)\n",
        "                        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "                        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "                        val_dataset = val_dataset.batch(batch_size)\n",
        "                        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "                        # Save fold-specific scalers if this becomes the best fold\n",
        "                        if fold_idx == 1 or best_val_metric is None:\n",
        "                            self.scaler = fold_scaler\n",
        "                            self.target_scaler = fold_target_scaler\n",
        "\n",
        "                        self.logger.info(\"Building model...\")\n",
        "                        model = self.build_model(X_train.shape[1], X_train.shape[2])\n",
        "                        if model is None:\n",
        "                            raise ValueError(\"Failed to build model\")\n",
        "\n",
        "                        # Create callbacks - FIX: moved after dataset creation\n",
        "                        callbacks = self.create_training_config(train_dataset)\n",
        "                        callbacks.extend([\n",
        "                            tf.keras.callbacks.CSVLogger(\n",
        "                                str(self.run_folder / f'training_log_fold_{fold_idx}.csv')\n",
        "                            ),\n",
        "                            MemoryMonitorCallback(self.logger)\n",
        "                        ])\n",
        "\n",
        "                        history = model.fit(\n",
        "                            train_dataset,\n",
        "                            epochs=100,\n",
        "                            validation_data=val_dataset,\n",
        "                            callbacks=callbacks,\n",
        "                            verbose=1\n",
        "                        )\n",
        "\n",
        "                        # Log early stopping information\n",
        "                        stopped_epoch = 0\n",
        "                        for callback in callbacks:\n",
        "                            if isinstance(callback, tf.keras.callbacks.EarlyStopping):\n",
        "                                stopped_epoch = callback.stopped_epoch\n",
        "                                break\n",
        "\n",
        "                        if stopped_epoch > 0:\n",
        "                            self.logger.info(f\"Early stopping triggered at epoch {stopped_epoch}\")\n",
        "                        else:\n",
        "                            self.logger.info(\"Training completed without early stopping\")\n",
        "                        histories.append(history.history)\n",
        "\n",
        "                        val_results = model.evaluate(val_dataset, verbose=1)\n",
        "\n",
        "                        fold_metric = {\n",
        "                            'fold': fold_idx,\n",
        "                            'val_loss': float(val_results[0]),\n",
        "                            'val_mae': float(val_results[1]),\n",
        "                            'val_mse': float(val_results[2])\n",
        "                        }\n",
        "                        fold_metrics.append(fold_metric)\n",
        "\n",
        "                        if val_results[0] < best_val_metric:\n",
        "                            best_val_metric = val_results[0]\n",
        "                            best_fold = fold_idx\n",
        "\n",
        "                            # Save best model\n",
        "                            best_model_dir = self.run_folder / 'best_model'\n",
        "                            best_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                            # Save model architecture\n",
        "                            model_json = model.to_json()\n",
        "                            with open(best_model_dir / 'model_architecture.json', 'w') as f:\n",
        "                                f.write(model_json)\n",
        "\n",
        "                            # Save weights with correct extension\n",
        "                            weights_path = str(best_model_dir / 'model.weights.h5')\n",
        "                            model.save_weights(weights_path)\n",
        "\n",
        "                            # Save fold indices\n",
        "                            np.save(str(best_model_dir / 'fold_indices.npy'),\n",
        "                                {'train': train_idx, 'val': val_idx})\n",
        "\n",
        "                            best_model = model\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.exception(f\"Error in fold {fold_idx}\")\n",
        "                    continue\n",
        "                finally:\n",
        "                    # Explicit cleanup\n",
        "                    tf.keras.backend.clear_session()\n",
        "                    if 'train_dataset' in locals():\n",
        "                        del train_dataset\n",
        "                    if 'val_dataset' in locals():\n",
        "                        del val_dataset\n",
        "                    import gc\n",
        "                    gc.collect()\n",
        "\n",
        "            # Save final metrics and create visualizations\n",
        "            if fold_metrics:\n",
        "                metrics_df = pd.DataFrame(fold_metrics)\n",
        "                metrics_df.to_csv(self.run_folder / 'fold_metrics.csv', index=False)\n",
        "\n",
        "                # Create performance plots\n",
        "                self.create_performance_plots(metrics_df, histories)\n",
        "\n",
        "                self.logger.info(\"\\nTraining Complete!\")\n",
        "                self.logger.info(\"=\" * 50)\n",
        "                self.logger.info(f\"Best fold: {best_fold}\")\n",
        "                self.logger.info(f\"Best validation loss: {best_val_metric:.4f}\")\n",
        "                self.logger.info(\"=\" * 50)\n",
        "\n",
        "            if best_model is None:\n",
        "                self.logger.error(\"All training folds failed. No best model available.\")\n",
        "                raise ValueError(\"Training failed - no best model available\")\n",
        "\n",
        "            return best_model\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error in train_and_evaluate\")\n",
        "            raise  # Re-raise the exception to ensure proper error handling\n",
        "\n",
        "\n",
        "    def create_performance_plots(self, metrics_df: pd.DataFrame, histories: list):\n",
        "        \"\"\"Create and save performance visualization plots.\"\"\"\n",
        "        try:\n",
        "            plots_dir = self.run_folder / 'plots'\n",
        "            plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            # Plot fold metrics\n",
        "            metrics = ['val_loss', 'val_mae', 'val_mse']\n",
        "            for metric in metrics:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.bar(metrics_df['fold'], metrics_df[metric])\n",
        "                plt.title(f'{metric} by Fold')\n",
        "                plt.xlabel('Fold')\n",
        "                plt.ylabel(metric)\n",
        "                plt.savefig(plots_dir / f'{metric}_by_fold.png')\n",
        "                plt.close()\n",
        "\n",
        "            # Plot training history\n",
        "            for history in histories:\n",
        "                for metric in history.keys():\n",
        "                    # Handle 'val_' prefixed metrics\n",
        "                    if metric.startswith('val_') or metric == 'loss':\n",
        "                        plt.figure(figsize=(10, 6))\n",
        "                        # Plot training metric (without 'val_')\n",
        "                        if metric.startswith('val_'):\n",
        "                          train_metric = metric.replace('val_', '')\n",
        "                          if train_metric in history:\n",
        "                            plt.plot(history[train_metric], label='train')\n",
        "                            plt.plot(history[metric], label='validation') #plot validation\n",
        "                          else:\n",
        "                           continue\n",
        "                        else:\n",
        "                            plt.plot(history[metric], label='train')\n",
        "\n",
        "                            val_metric = 'val_' + metric\n",
        "                            if val_metric in history:\n",
        "                                plt.plot(history[val_metric], label='validation')\n",
        "\n",
        "                        plt.title(f'{metric} During Training')\n",
        "                        plt.xlabel('Epoch')\n",
        "                        plt.ylabel(metric)\n",
        "                        plt.legend()\n",
        "                        plt.savefig(plots_dir / f'{metric}_history.png')\n",
        "                        plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(\"Error creating performance plots\")\n",
        "\n",
        "    def analyze_participant(self, pid: str, model: tf.keras.Model) -> None:\n",
        "        \"\"\"Generate prediction plots for a specific participant with proper alignment.\"\"\"\n",
        "        try:\n",
        "            dfp = self.data[self.data['participant_id'] == pid].copy()\n",
        "            if dfp.empty:\n",
        "                self.logger.warning(f\"No data for participant {pid}\")\n",
        "                return\n",
        "\n",
        "            # Create sequences with timestamp tracking\n",
        "            feature_data = dfp[self.features].values\n",
        "            n_samples = len(dfp)\n",
        "            if n_samples < self.sequence_length:\n",
        "                self.logger.warning(f\"Insufficient samples for participant {pid}\")\n",
        "                return\n",
        "\n",
        "            n_sequences = (n_samples - self.sequence_length + self.stride) // self.stride\n",
        "            sequences = np.zeros((n_sequences, self.sequence_length, len(self.features)))\n",
        "            target_indices = []  # Track exact indices for alignment\n",
        "\n",
        "            for i in range(n_sequences):\n",
        "                start_idx = i * self.stride\n",
        "                end_idx = start_idx + self.sequence_length\n",
        "                sequences[i] = feature_data[start_idx:end_idx]\n",
        "                target_indices.append(end_idx - 1)  # Index where prediction applies\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = model.predict(sequences)\n",
        "            predictions = self.target_scaler.inverse_transform(predictions)\n",
        "\n",
        "            # Get actual values at exactly the same indices\n",
        "            actual_indices = np.array(target_indices)\n",
        "            actual_values = dfp[self.target].values[actual_indices].reshape(-1, 1)\n",
        "            actual_values = self.target_scaler.inverse_transform(actual_values)\n",
        "\n",
        "            # Get timestamps at those exact indices\n",
        "            timestamps = dfp[self.timestamp_column].values[actual_indices]\n",
        "\n",
        "            # Plot with perfect alignment\n",
        "            plt.figure(figsize=(15, 7))\n",
        "            plt.plot(timestamps, predictions, label='Predicted', alpha=0.7)\n",
        "            plt.plot(timestamps, actual_values, label='Actual', alpha=0.7)\n",
        "\n",
        "            # Calculate metrics with perfectly aligned data\n",
        "            mae = np.mean(np.abs(predictions - actual_values))\n",
        "            mse = np.mean((predictions - actual_values) ** 2)\n",
        "            rmse = np.sqrt(mse)\n",
        "\n",
        "            metrics = {\n",
        "                'participant_id': pid,\n",
        "                'mae': float(mae),\n",
        "                'mse': float(mse),\n",
        "                'rmse': float(rmse)\n",
        "            }\n",
        "\n",
        "            with open(self.run_folder / 'participant_plots' / f'{pid}_metrics.json', 'w') as f:\n",
        "                json.dump(metrics, f, indent=4)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.exception(f\"Error analyzing participant {pid}\")\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Main execution function with improved stability measures.\"\"\"\n",
        "    try:\n",
        "        # Initialize system with stable configuration\n",
        "        system = GazeAnalysisSystem(\n",
        "            window_size=50,\n",
        "            stride=9,\n",
        "            model_type=\"cnn\",\n",
        "            use_robust_scaler=True,\n",
        "            clip_range=(-3, 3),\n",
        "            max_seq_per_participant=50000\n",
        "        )\n",
        "\n",
        "        # Execute pipeline with proper logging\n",
        "        logger = logging.getLogger('GazeAnalysisSystem')\n",
        "        logger.info(\"Phase 1: Load & preprocess data\")\n",
        "        system.load_and_preprocess_data()\n",
        "\n",
        "        logger.info(\"Phase 2: Train & evaluate\")\n",
        "        model = system.train_and_evaluate()\n",
        "\n",
        "        # Check if model training was successful before proceeding\n",
        "        if model is None:\n",
        "            logger.error(\"Model training failed. Cannot analyze participants.\")\n",
        "            return  # Exit if no model was trained\n",
        "\n",
        "\n",
        "        logger.info(\"Phase 3: Evaluate participant generalization\")\n",
        "        generalization_metrics = system.evaluate_participant_generalization()\n",
        "        logger.info(f\"Generalization metrics saved to {system.run_folder / 'participant_generalization.csv'}\")\n",
        "\n",
        "        logger.info(\"Phase 4: Analyze participants\")\n",
        "        participant_ids = system.data['participant_id'].unique()\n",
        "        for pid in participant_ids:\n",
        "            system.analyze_participant(pid, model)\n",
        "\n",
        "        logger.info(f\"Analysis complete. Results saved in {system.run_folder}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error in main execution\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_9SO9wnbmXa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "def find_best_weights(output_dir: str):\n",
        "    \"\"\"\n",
        "    Finds the best weights file in the checkpoints directory.\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    checkpoints_dir = output_dir / 'checkpoints'\n",
        "\n",
        "    if not checkpoints_dir.exists():\n",
        "        print(\"No checkpoints directory found\")\n",
        "        return None\n",
        "\n",
        "    # Get all weight files and parse their numbers correctly\n",
        "    weight_files = []\n",
        "    for f in checkpoints_dir.glob('model_*.weights.h5'):\n",
        "        try:\n",
        "            # Extract the number between 'model_' and '.weights.h5'\n",
        "            num = int(f.name.replace('model_', '').replace('.weights.h5', ''))\n",
        "            weight_files.append((num, f))\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file with invalid format: {f.name}\")\n",
        "\n",
        "    if not weight_files:\n",
        "        print(\"No valid weight files found in checkpoints\")\n",
        "        return None\n",
        "\n",
        "    # Sort by epoch number and get the latest\n",
        "    weight_files.sort(key=lambda x: x[0])  # Sort by the extracted number\n",
        "    best_weights = weight_files[-1][1]  # Take the file path from the tuple with highest number\n",
        "\n",
        "    print(f\"Found best weights file: {best_weights.name}\")\n",
        "    return best_weights\n",
        "\n",
        "def load_model_safely(output_dir: str, model_type='cnn_transformer'):\n",
        "    \"\"\"\n",
        "    Enhanced model loading with proper weight file handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Initializing model loading process...\")\n",
        "        output_dir = Path(output_dir)\n",
        "\n",
        "        # Load model architecture\n",
        "        architecture_path = output_dir / 'best_model' / 'model_architecture.json'\n",
        "        if architecture_path.exists():\n",
        "            print(\"Found model architecture file\")\n",
        "            with open(architecture_path, 'r') as f:\n",
        "                model_json = f.read()\n",
        "                model = tf.keras.models.model_from_json(model_json)\n",
        "                print(\"Successfully loaded model architecture\")\n",
        "        else:\n",
        "            print(\"No architecture file found, rebuilding model...\")\n",
        "            return None\n",
        "\n",
        "        # Find and load best weights\n",
        "        best_weights = find_best_weights(output_dir)\n",
        "        if best_weights:\n",
        "            try:\n",
        "                model.load_weights(str(best_weights))\n",
        "                print(f\"Successfully loaded weights from {best_weights.name}\")\n",
        "\n",
        "                # Compile the model after loading weights\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=0.5)\n",
        "                model.compile(\n",
        "                    optimizer=optimizer,\n",
        "                    loss='mean_squared_error',\n",
        "                    metrics=['mean_absolute_error', 'mean_squared_error']\n",
        "                )\n",
        "                print(\"Model compiled successfully\")\n",
        "\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading weights: {str(e)}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\"No weights found\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model loading: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Test the loading\n",
        "run_dir = '/content/drive/MyDrive/output/run_20250225_184902'\n",
        "model = load_model_safely(run_dir)\n",
        "\n",
        "if model:\n",
        "    print(\"\\nModel loaded successfully!\")\n",
        "    print(\"Model summary:\")\n",
        "    model.summary()\n",
        "else:\n",
        "    print(\"\\nFailed to load model properly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaqSqt1uuunf"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import traceback\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from scipy.interpolate import interp1d\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--drive_path', type=str, default='/content/drive/MyDrive/pilotdata',\n",
        "                      help=\"Path to gaze CSV files\")\n",
        "    parser.add_argument('--output_dir', type=str,\n",
        "                      default='/content/drive/MyDrive/output/run_20250225_184902',  # Change this line\n",
        "                      help=\"Path to find the best_model folder\")\n",
        "    parser.add_argument('--model_type', type=str, default='cnn_transformer',\n",
        "                      choices=['cnn', 'lstm', 'rnn', 'cnn_transformer'],\n",
        "                      help=\"Type of model architecture\")\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def rebuild_model(sequence_length=50, n_features=17, model_type='cnn_transformer'):\n",
        "    \"\"\"Rebuilds model using specified architecture.\"\"\"\n",
        "    try:\n",
        "        inputs = tf.keras.layers.Input(shape=(sequence_length, n_features))\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        if model_type == 'cnn':\n",
        "            # CNN Architecture\n",
        "            x = tf.keras.layers.Conv1D(32, kernel_size=3, padding=\"same\")(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "            x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "            x = tf.keras.layers.Conv1D(64, kernel_size=3, padding=\"same\")(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "            x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "            x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "\n",
        "        elif model_type == 'lstm':\n",
        "            # LSTM Architecture\n",
        "            x = tf.keras.layers.LSTM(32, return_sequences=True)(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "            x = tf.keras.layers.LSTM(16)(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "\n",
        "        elif model_type == 'rnn':\n",
        "            # RNN Architecture\n",
        "            x = tf.keras.layers.SimpleRNN(32, return_sequences=True)(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "            x = tf.keras.layers.SimpleRNN(16)(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "\n",
        "        else:\n",
        "            # CNN-Transformer Architecture\n",
        "            x = tf.keras.layers.Dense(32)(x)\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "            x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "            for filters in [32, 64]:\n",
        "                x = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, padding=\"same\")(x)\n",
        "                x = tf.keras.layers.BatchNormalization()(x)\n",
        "                x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "                x = tf.keras.layers.Activation('relu')(x)\n",
        "                x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "            attention_output = tf.keras.layers.MultiHeadAttention(\n",
        "                key_dim=32, num_heads=4, dropout=0.1\n",
        "            )(x, x)\n",
        "            x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output * 0.5)\n",
        "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "            x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        optimizer = Adam(learning_rate=1e-5, clipnorm=0.5)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='mean_squared_error',\n",
        "            metrics=['mean_absolute_error', 'mean_squared_error']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error rebuilding model: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def find_best_weights(output_dir: str):\n",
        "    \"\"\"\n",
        "    Finds the best weights file in the checkpoints directory.\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    checkpoints_dir = output_dir / 'checkpoints'\n",
        "\n",
        "    if not checkpoints_dir.exists():\n",
        "        print(\"No checkpoints directory found\")\n",
        "        return None\n",
        "\n",
        "    # Get all weight files and parse their numbers correctly\n",
        "    weight_files = []\n",
        "    for f in checkpoints_dir.glob('model_*.weights.h5'):\n",
        "        try:\n",
        "            # Extract the number between 'model_' and '.weights.h5'\n",
        "            num = int(f.name.replace('model_', '').replace('.weights.h5', ''))\n",
        "            weight_files.append((num, f))\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file with invalid format: {f.name}\")\n",
        "\n",
        "    if not weight_files:\n",
        "        print(\"No valid weight files found in checkpoints\")\n",
        "        return None\n",
        "\n",
        "    # Sort by epoch number and get the latest\n",
        "    weight_files.sort(key=lambda x: x[0])  # Sort by the extracted number\n",
        "    best_weights = weight_files[-1][1]  # Take the file path from the tuple with highest number\n",
        "\n",
        "    print(f\"Found best weights file: {best_weights.name}\")\n",
        "    return best_weights\n",
        "\n",
        "def load_model_safely(output_dir, model_type='cnn_transformer'):\n",
        "    \"\"\"Enhanced model loading with architecture support.\"\"\"\n",
        "    try:\n",
        "        print(\"Initializing model loading process...\")\n",
        "        output_dir = Path(output_dir)\n",
        "\n",
        "        # Load model architecture\n",
        "        architecture_path = output_dir / 'best_model' / 'model_architecture.json'\n",
        "        if architecture_path.exists():\n",
        "            print(\"Found model architecture file\")\n",
        "            with open(architecture_path, 'r') as f:\n",
        "                model_json = f.read()\n",
        "                model = tf.keras.models.model_from_json(model_json)\n",
        "                print(\"Successfully loaded model architecture\")\n",
        "        else:\n",
        "            print(\"No architecture file found, rebuilding model...\")\n",
        "            # Fall back to rebuilding model\n",
        "            model = rebuild_model(\n",
        "                sequence_length=50,\n",
        "                n_features=17,\n",
        "                model_type=model_type\n",
        "            )\n",
        "            if model is None:\n",
        "                raise ValueError(\"Failed to rebuild model\")\n",
        "\n",
        "        # Find and load best weights\n",
        "        best_weights = find_best_weights(output_dir)\n",
        "        if best_weights:\n",
        "            try:\n",
        "                model.load_weights(str(best_weights))\n",
        "                print(f\"Successfully loaded weights from {best_weights.name}\")\n",
        "\n",
        "                # Compile the model after loading weights\n",
        "                optimizer = Adam(learning_rate=1e-5, clipnorm=0.5)\n",
        "                model.compile(\n",
        "                    optimizer=optimizer,\n",
        "                    loss='mean_squared_error',\n",
        "                    metrics=['mean_absolute_error', 'mean_squared_error']\n",
        "                )\n",
        "                print(\"Model compiled successfully\")\n",
        "\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading weights: {str(e)}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\"No weights found\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error in model loading: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def normalize_scores(scores):\n",
        "    \"\"\"Normalize numeric scores to 0-1 range.\"\"\"\n",
        "    scores = np.array(scores, dtype=np.float32)\n",
        "    if scores.max() > 1.0:\n",
        "        return scores / 100.0\n",
        "    return scores\n",
        "\n",
        "def make_predictions_safely(model, sequences, batch_size=128):\n",
        "    \"\"\"Memory-efficient batch predictions.\"\"\"\n",
        "    try:\n",
        "        sequences = np.array(sequences, dtype=np.float32)\n",
        "        if sequences.ndim != 3:\n",
        "            raise ValueError(f\"Expected 3D sequences, got shape {sequences.shape}\")\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(0, len(sequences), batch_size):\n",
        "            batch = sequences[i:i + batch_size]\n",
        "            batch_predictions = model.predict(batch, verbose=0)\n",
        "            predictions.append(batch_predictions)\n",
        "\n",
        "        predictions = np.concatenate(predictions, axis=0).flatten()\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error making predictions: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def create_sequences(df, features, sequence_length):\n",
        "    \"\"\"Memory-efficient sequence creation.\"\"\"\n",
        "    try:\n",
        "        data_array = df[features].values.astype(np.float32)\n",
        "        n_samples = len(data_array)\n",
        "\n",
        "        if n_samples < sequence_length:\n",
        "            raise ValueError(f\"Not enough samples ({n_samples}) for sequence length {sequence_length}\")\n",
        "\n",
        "        # Use numpy's strided array for memory efficiency\n",
        "        s = data_array.strides\n",
        "        sequences = np.lib.stride_tricks.as_strided(\n",
        "            data_array,\n",
        "            shape=(n_samples - sequence_length + 1, sequence_length, len(features)),\n",
        "            strides=(s[0], s[0], s[1]),\n",
        "            writeable=False\n",
        "        )\n",
        "\n",
        "        return sequences.copy()  # Return a contiguous copy\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating sequences: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "def analyze_eye_movements_with_phases(drive_path, output_dir, model_type='cnn_transformer'):\n",
        "    \"\"\"Main analysis function with specified model architecture support.\"\"\"\n",
        "    try:\n",
        "        # Initialize paths\n",
        "        drive_path = Path(drive_path)\n",
        "        output_dir = Path('/content/drive/MyDrive/output/run_20250225_184902')\n",
        "        base_dir = Path('/content/drive/MyDrive/output')  # Base directory for participant data\n",
        "        movement_dir = output_dir / 'movement_analysis_with_phases'\n",
        "        movement_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Create data directory for CSVs\n",
        "        data_dir = movement_dir / 'data'\n",
        "        data_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Load model and scaler\n",
        "        model = load_model_safely(output_dir, model_type)\n",
        "        if model is None:\n",
        "            print(\"Failed to load model. Cannot proceed.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            scaler = joblib.load(output_dir / 'feature_scaler.gz')\n",
        "            print(\"Loaded feature scaler\")\n",
        "        except Exception as e:\n",
        "            print(f\"No scaler found: {str(e)}\")\n",
        "            scaler = None\n",
        "\n",
        "        features = [\n",
        "            'FPOGX', 'FPOGY', 'FPOGD',\n",
        "            'BPOGX', 'BPOGY',\n",
        "            'LPCX', 'LPCY', 'LPD', 'LPS',\n",
        "            'RPCX', 'RPCY', 'RPD', 'RPS',\n",
        "            'LPMM', 'RPMM',\n",
        "            'CX', 'CY'\n",
        "        ]\n",
        "\n",
        "        # Get participant list\n",
        "        gaze_files = list(drive_path.glob('*_all_gaze.csv'))\n",
        "        if not gaze_files:\n",
        "            print(f\"No gaze files found in {drive_path}\")\n",
        "            return\n",
        "\n",
        "        participant_ids = [f.stem.split('_')[0] for f in gaze_files]\n",
        "        print(f\"Found {len(participant_ids)} participants to analyze\")\n",
        "\n",
        "        # Analysis storage\n",
        "        all_times = []\n",
        "        all_predictions = []\n",
        "        all_actual_scores = []\n",
        "        all_phases = []\n",
        "        successful_analyses = []\n",
        "        failed_analyses = []\n",
        "\n",
        "        def determine_success(predictions, threshold=0.7, sustained_period=300):\n",
        "            \"\"\"\n",
        "            Determine if the performance is successful based on:\n",
        "            1. Achieving above threshold performance\n",
        "            2. Maintaining it for a sustained period\n",
        "            3. Considering the overall trend\n",
        "            \"\"\"\n",
        "            above_threshold = predictions >= threshold\n",
        "\n",
        "            # Find the longest sustained period above threshold\n",
        "            max_sustained = 0\n",
        "            current_sustained = 0\n",
        "            success_start_time = None\n",
        "\n",
        "            for i, is_above in enumerate(above_threshold):\n",
        "                if is_above:\n",
        "                    current_sustained += 1\n",
        "                    if current_sustained > max_sustained:\n",
        "                        max_sustained = current_sustained\n",
        "                        if success_start_time is None:\n",
        "                            success_start_time = i\n",
        "                else:\n",
        "                    current_sustained = 0\n",
        "\n",
        "            # Calculate the percentage of time spent above threshold\n",
        "            time_above_threshold = np.mean(above_threshold)\n",
        "\n",
        "            # Consider it successful if:\n",
        "            # 1. Has a sustained period above threshold\n",
        "            # 2. Spends at least 30% of total time above threshold\n",
        "            is_successful = (max_sustained >= sustained_period) and (time_above_threshold >= 0.3)\n",
        "\n",
        "            return is_successful, (success_start_time if success_start_time else 0) / 60.0  # Convert to minutes\n",
        "\n",
        "        # Process each participant\n",
        "        for participant_id in participant_ids:\n",
        "            try:\n",
        "                print(f\"\\nProcessing participant {participant_id}...\")\n",
        "\n",
        "                # Update paths to look in the correct locations\n",
        "                gaze_file = drive_path / f\"{participant_id}_all_gaze.csv\"\n",
        "                scores_file = base_dir / participant_id / f\"{participant_id}_detailed_scores.csv\"\n",
        "\n",
        "                if not gaze_file.exists():\n",
        "                    print(f\"Missing gaze file: {gaze_file}\")\n",
        "                    continue\n",
        "                if not scores_file.exists():\n",
        "                    print(f\"Missing scores file: {scores_file}\")\n",
        "                    continue\n",
        "\n",
        "                # Load and preprocess data\n",
        "                df_gaze = pd.read_csv(gaze_file)\n",
        "                df_scores = pd.read_csv(scores_file)\n",
        "\n",
        "                # Process timestamps\n",
        "                time_cols = [col for col in df_gaze.columns if col.startswith('TIME(')]\n",
        "                if not time_cols:\n",
        "                    print(f\"No TIME column found for {participant_id}\")\n",
        "                    continue\n",
        "\n",
        "                df_gaze['TIMESTAMP'] = df_gaze[time_cols[0]] - df_gaze[time_cols[0]].iloc[0]\n",
        "                df_scores['TIMESTAMP'] = df_scores['Time'] - df_scores['Time'].iloc[0]\n",
        "\n",
        "                # Merge data\n",
        "                df_merged = pd.merge_asof(\n",
        "                    df_gaze.sort_values('TIMESTAMP'),\n",
        "                    df_scores[['TIMESTAMP', 'Phase', 'Row_Total_Score']].sort_values('TIMESTAMP'),\n",
        "                    on='TIMESTAMP',\n",
        "                    direction='nearest',\n",
        "                    tolerance=1.0\n",
        "                )\n",
        "\n",
        "                # Clean and prepare\n",
        "                df_merged = df_merged.dropna(subset=['Row_Total_Score'])\n",
        "                df_merged[features] = df_merged[features].ffill().bfill().fillna(0)\n",
        "\n",
        "                # Scale features if scaler available\n",
        "                if scaler is not None:\n",
        "                    df_merged[features] = scaler.transform(df_merged[features])\n",
        "\n",
        "                # Normalize scores\n",
        "                df_merged['Row_Total_Score'] = normalize_scores(df_merged['Row_Total_Score'])\n",
        "\n",
        "                # Create sequences\n",
        "                sequence_length = 50\n",
        "                X_sequences = create_sequences(df_merged, features, sequence_length)\n",
        "\n",
        "                # Make predictions\n",
        "                predictions = make_predictions_safely(model, X_sequences)\n",
        "                if predictions is None:\n",
        "                    print(f\"Skipping participant {participant_id} due to prediction error\")\n",
        "                    continue\n",
        "\n",
        "                # Align predictions with data\n",
        "                df_merged = df_merged.iloc[sequence_length - 1:].copy()\n",
        "                df_merged['Predicted_Score'] = predictions\n",
        "                df_merged['Time_Minutes'] = df_merged['TIMESTAMP'] / 60.0\n",
        "\n",
        "                times = df_merged['Time_Minutes'].values\n",
        "                phases = df_merged['Phase'].values\n",
        "                actual_scores = df_merged['Row_Total_Score'].values\n",
        "                predictions = df_merged['Predicted_Score'].values\n",
        "\n",
        "                # Save participant data to CSV\n",
        "                participant_data = pd.DataFrame({\n",
        "                    'Time_Minutes': times,\n",
        "                    'Predicted_Score': predictions,\n",
        "                    'Actual_Score': actual_scores,\n",
        "                    'Phase': phases\n",
        "                })\n",
        "\n",
        "                # Determine success and get the critical time point\n",
        "                is_successful, success_time = determine_success(predictions)\n",
        "                success_status = \"Successful\" if is_successful else \"Unsuccessful\"\n",
        "\n",
        "                # Add success information to the dataframe\n",
        "                participant_data['Success_Status'] = success_status\n",
        "                participant_data['Success_Time'] = success_time if is_successful else None\n",
        "\n",
        "                # Save detailed participant data\n",
        "                csv_filename = f'{participant_id}_movement_analysis_data.csv'\n",
        "                participant_data.to_csv(data_dir / csv_filename, index=False)\n",
        "                print(f\"Saved data for participant {participant_id} to {csv_filename}\")\n",
        "\n",
        "                # Create plot\n",
        "                plt.figure(figsize=(15, 6))\n",
        "\n",
        "                plt.plot(times, predictions, '-', color='blue', alpha=0.7, label='Predicted Score')\n",
        "                plt.plot(times, actual_scores, '-', color='green', alpha=0.7, label='Actual Score')\n",
        "\n",
        "                # Add threshold line\n",
        "                plt.axhline(y=0.7, color='red', linestyle='--', label='Success Threshold (0.7)')\n",
        "\n",
        "                # If successful, mark the success point\n",
        "                if is_successful:\n",
        "                    plt.axvline(x=success_time, color='green', linestyle=':', label=f'Success Point: {success_time:.2f} min')\n",
        "\n",
        "                plt.ylim(0, 1.0)\n",
        "                plt.ylabel('Score (0-1)')\n",
        "\n",
        "                # Mark phases\n",
        "                phase_changes = df_merged['Phase'].ne(df_merged['Phase'].shift()).cumsum()\n",
        "                phase_boundaries = df_merged.groupby(phase_changes)['Time_Minutes'].agg(['first', 'last']).reset_index(drop=True)\n",
        "                for idx, row in phase_boundaries.iterrows():\n",
        "                    start = row['first']\n",
        "                    end = row['last']\n",
        "                    phase_name = df_merged.loc[df_merged['Time_Minutes'] == start, 'Phase'].values[0]\n",
        "                    plt.axvspan(start, end, color='yellow', alpha=0.1)\n",
        "                    plt.text((start + end) / 2, 0.98, phase_name,\n",
        "                            ha='center', va='top', fontsize=9)\n",
        "\n",
        "                plt.title(f'Eye Movement Timeline - Participant {participant_id} ({success_status})')\n",
        "                plt.xlabel('Time (minutes)')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save plot\n",
        "                plot_filename = f'{participant_id}_movement_analysis_success.png'\n",
        "                plt.savefig(movement_dir / plot_filename)\n",
        "                plt.close()\n",
        "                print(f\"Saved plot for participant {participant_id} - {success_status}\")\n",
        "\n",
        "                # Store data for final summary\n",
        "                all_times.append(times)\n",
        "                all_predictions.append(predictions)\n",
        "                all_actual_scores.append(actual_scores)\n",
        "                all_phases.append(phases)\n",
        "                successful_analyses.append(participant_id)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing participant {participant_id}: {str(e)}\")\n",
        "                failed_analyses.append(participant_id)\n",
        "                continue\n",
        "\n",
        "        # Post-processing across all participants\n",
        "        if len(all_times) > 0:\n",
        "            max_time = max([t.max() for t in all_times])\n",
        "            time_grid = np.linspace(0, max_time, num=1000)\n",
        "\n",
        "            # Interpolate predictions\n",
        "            interpolated_predictions = []\n",
        "            for times, preds in zip(all_times, all_predictions):\n",
        "                f = interp1d(times, preds, kind='linear', bounds_error=False, fill_value=np.nan)\n",
        "                interp_pred = f(time_grid)\n",
        "                interpolated_predictions.append(interp_pred)\n",
        "\n",
        "            interpolated_predictions = np.array(interpolated_predictions)\n",
        "            mean_predictions = np.nanmean(interpolated_predictions, axis=0)\n",
        "            std_predictions = np.nanstd(interpolated_predictions, axis=0)\n",
        "\n",
        "            # Save interpolated data for all participants\n",
        "            interpolated_data = pd.DataFrame({\n",
        "                'Time_Minutes': time_grid,\n",
        "                'Mean_Predicted_Score': mean_predictions,\n",
        "                'Standard_Deviation': std_predictions,\n",
        "                'Number_of_Participants': np.sum(~np.isnan(interpolated_predictions), axis=0)\n",
        "            })\n",
        "\n",
        "            # Add individual participant data as additional columns\n",
        "            for i, participant_id in enumerate(successful_analyses):\n",
        "                interpolated_data[f'Participant_{participant_id}'] = interpolated_predictions[i]\n",
        "\n",
        "            # Save detailed interpolated data\n",
        "            interpolated_data.to_csv(data_dir / 'all_participants_interpolated_data.csv', index=False)\n",
        "            print(f\"Saved interpolated data for all participants\")\n",
        "\n",
        "            # Find peak\n",
        "            peak_index = np.nanargmax(mean_predictions)\n",
        "            peak_time = time_grid[peak_index]\n",
        "            peak_value = mean_predictions[peak_index]\n",
        "            print(f\"\\nPeak prediction at {peak_time:.2f} min with value {peak_value:.2f}\")\n",
        "\n",
        "            # Calculate correlation\n",
        "            all_actual_flat = np.concatenate(all_actual_scores)\n",
        "            all_preds_flat = np.concatenate(all_predictions)\n",
        "            correlation = np.corrcoef(all_actual_flat, all_preds_flat)[0, 1]\n",
        "\n",
        "            # Save summary statistics\n",
        "            summary_stats = {\n",
        "                'Total_Participants': len(successful_analyses),\n",
        "                'Failed_Participants': len(failed_analyses),\n",
        "                'Peak_Time': peak_time,\n",
        "                'Peak_Value': peak_value,\n",
        "                'Overall_Correlation': correlation\n",
        "            }\n",
        "\n",
        "            # Add participant-wise success information\n",
        "            for participant_id in successful_analyses:\n",
        "                participant_data = pd.read_csv(data_dir / f'{participant_id}_movement_analysis_data.csv')\n",
        "                success_status = participant_data['Success_Status'].iloc[0]\n",
        "                success_time = participant_data['Success_Time'].iloc[0]\n",
        "                summary_stats[f'{participant_id}_Success'] = success_status\n",
        "                summary_stats[f'{participant_id}_Success_Time'] = success_time\n",
        "\n",
        "            # Save summary statistics\n",
        "            pd.DataFrame([summary_stats]).to_csv(data_dir / 'analysis_summary.csv', index=False)\n",
        "            print(\"Saved analysis summary\")\n",
        "\n",
        "            # Plot and save average predictions\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(time_grid, mean_predictions, color='blue', label='Avg Predicted Score')\n",
        "            plt.fill_between(time_grid,\n",
        "                           mean_predictions - std_predictions,\n",
        "                           mean_predictions + std_predictions,\n",
        "                           color='blue', alpha=0.2, label='±1 SD')\n",
        "            plt.axhline(y=0.7, color='red', linestyle='--', label='Success Threshold (0.7)')\n",
        "            plt.plot(peak_time, peak_value, 'ko', label=f'Peak={peak_time:.2f} min')\n",
        "            plt.title('Average Predicted Score Over Time')\n",
        "            plt.xlabel('Time (minutes)')\n",
        "            plt.ylabel('Score (0-1)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.ylim(0, 1.0)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save average predictions plot\n",
        "            plt.savefig(movement_dir / 'average_prediction_over_time.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Save average predictions plot data\n",
        "            average_plot_data = pd.DataFrame({\n",
        "                'Time_Minutes': time_grid,\n",
        "                'Mean_Predicted_Score': mean_predictions,\n",
        "                'Standard_Deviation': std_predictions,\n",
        "                'Success_Threshold': np.full_like(time_grid, 0.7),\n",
        "                'Peak_Time': peak_time,\n",
        "                'Peak_Value': peak_value\n",
        "            })\n",
        "            average_plot_data.to_csv(data_dir / 'average_predictions_plot_data.csv', index=False)\n",
        "            print(\"Saved average predictions plot data\")\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nMovement analysis complete!\")\n",
        "        print(f\"Results saved in: {movement_dir}\")\n",
        "        print(\"Analysis Complete:\")\n",
        "        print(f\"Successfully analyzed: {len(successful_analyses)} participants\")\n",
        "        print(f\"Failed to analyze: {len(failed_analyses)} participants\")\n",
        "\n",
        "        if failed_analyses:\n",
        "            print(\"\\nFailed participants:\")\n",
        "            for pid in failed_analyses:\n",
        "                print(f\"- {pid}\")\n",
        "\n",
        "        # Save final analysis status to CSV\n",
        "        final_status = {\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'total_participants': len(participant_ids),\n",
        "            'successful_analyses': len(successful_analyses),\n",
        "            'failed_analyses': len(failed_analyses),\n",
        "            'failed_participant_ids': ','.join(failed_analyses) if failed_analyses else 'None'\n",
        "        }\n",
        "        pd.DataFrame([final_status]).to_csv(data_dir / 'final_analysis_status.csv', index=False)\n",
        "        print(\"\\nFinal analysis status saved to final_analysis_status.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error in analysis: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Save error information\n",
        "        try:\n",
        "            error_info = {\n",
        "                'error_message': str(e),\n",
        "                'traceback': traceback.format_exc(),\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            # Ensure data directory exists even if error occurred early\n",
        "            data_dir = output_dir / 'movement_analysis_with_phases' / 'data'\n",
        "            data_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "            # Save error log to CSV\n",
        "            pd.DataFrame([error_info]).to_csv(data_dir / 'error_log.csv', index=False)\n",
        "            print(\"Error information saved to error_log.csv\")\n",
        "        except Exception as inner_e:\n",
        "            print(f\"Failed to save error information: {str(inner_e)}\")\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    analyze_eye_movements_with_phases(args.drive_path, args.output_dir, args.model_type)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}